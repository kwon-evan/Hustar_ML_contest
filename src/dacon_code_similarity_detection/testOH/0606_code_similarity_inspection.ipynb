{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdccdb56-b73a-44e9-acef-5263fc1d3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57da6d25-fa8a-4fcc-9230-945c173a8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_folder = \"./code\"\n",
    "problem_folders = os.listdir(code_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9318d9ba-5e24-4bc8-8be5-e413d09e4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:09<00:00, 32.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_script(script):\n",
    "    with open(script, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        preproc_lines = list()\n",
    "        for line in lines:\n",
    "            if line.lstrip().startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.rstrip()\n",
    "            if \"#\" in line:\n",
    "                line = line[:line.index(\"#\")]\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            line = line.replace(\"    \", \"\\t\")\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            preproc_lines.append(line)\n",
    "        preprocessed_script = \"\\n\".join(preproc_lines)\n",
    "    return preprocessed_script\n",
    "\n",
    "preproc_scripts = list()\n",
    "problem_nums = list()\n",
    "\n",
    "for problem_folder in tqdm(problem_folders):\n",
    "    scripts = os.listdir(os.path.join(code_folder, problem_folder))\n",
    "    problem_num = scripts[0].split(\"_\")[0]\n",
    "    for script in scripts:\n",
    "        script_file = os.path.join(code_folder, problem_folder, script)\n",
    "        preprocessed_script = preprocess_script(script_file)\n",
    "        \n",
    "        preproc_scripts.append(preprocessed_script)\n",
    "    problem_nums.extend([problem_num]*len(scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c3001f-7592-4cbc-86e3-ab40a0710fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data = {\"code\":preproc_scripts, \"problem_num\":problem_nums})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b014b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = dict()\n",
    "for code, prob_num in zip(df[\"code\"], df[\"problem_num\"]):\n",
    "    if prob_num in temp_dict:\n",
    "        temp_dict[prob_num] += 1\n",
    "    else:\n",
    "        temp_dict[prob_num] = 1\n",
    "\n",
    "# print(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c24572-eebc-4399-98a4-5bfa3aa3975a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeaf4555-712d-4799-8cac-ac3236ca1f13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3212/2923025650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncation_side\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"len\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
    "tokenizer.truncation_side = \"left\"\n",
    "df[\"tokens\"] = df[\"code\"].apply(tokenizer.tokenize)\n",
    "df[\"len\"] = df[\"tokens\"].apply(len)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ed19cf-62d0-4f99-ad4d-f435beebce4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43647.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>137.920842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.933475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>187.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>512.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                len\n",
       "count  43647.000000\n",
       "mean     137.920842\n",
       "std      104.933475\n",
       "min        5.000000\n",
       "25%       60.000000\n",
       "50%      104.000000\n",
       "75%      187.000000\n",
       "max      512.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = df[df[\"len\"] <= 512].reset_index(drop=True)\n",
    "ndf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c686693-1df2-477c-a117-89ebc043976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f28eb1e-c872-40bc-bdb3-767b4d952d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    179700.000000\n",
       "mean        392.408347\n",
       "std         923.698933\n",
       "min          20.000000\n",
       "25%         153.000000\n",
       "50%         255.000000\n",
       "75%         489.000000\n",
       "max      203699.000000\n",
       "Name: code1_len, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame[\"code1_len\"] = data_frame[\"code1\"].apply(len)\n",
    "data_frame[\"code1_len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b21656d-7b75-4d45-9b0a-b9104e962b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    179700.000000\n",
       "mean        390.010367\n",
       "std        1333.079216\n",
       "min          15.000000\n",
       "25%         146.000000\n",
       "50%         254.000000\n",
       "75%         477.000000\n",
       "max      203669.000000\n",
       "Name: code2_len, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame[\"code2_len\"] = data_frame[\"code2\"].apply(len)\n",
    "data_frame[\"code2_len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05512b08-2ae0-45f4-88a9-51a6c305e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df, train_label, valid_label = train_test_split(\n",
    "    ndf,\n",
    "    ndf[\"problem_num\"],\n",
    "    random_state=42,\n",
    "    test_size=0.1,\n",
    "    stratify=ndf[\"problem_num\"],\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "805610e6-369e-4903-8dfa-481af9691617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159cc5e4-3a67-4b04-8456-bd102bae75f9",
   "metadata": {},
   "source": [
    "#### Train negative pair 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed7de96-95fe-4249-9578-dfdad15ccf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [12:53<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "codes = train_df[\"code\"].to_list()\n",
    "problems = train_df[\"problem_num\"].unique().tolist()\n",
    "problems.sort()\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "total_positive_pairs = list()\n",
    "total_negative_pairs = list()\n",
    "\n",
    "for problem in tqdm(problems):\n",
    "    solution_codes = train_df[train_df[\"problem_num\"] == problem][\"code\"]\n",
    "    positive_pairs = list(combinations(solution_codes.to_list(), 2))\n",
    "    \n",
    "    solution_codes_indices = solution_codes.index.to_list()\n",
    "    negative_pairs = list()\n",
    "    \n",
    "    first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "    negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "    negative_code_ranking = negative_code_scores.argsort()[::-1]\n",
    "    ranking_idx = 0\n",
    "    \n",
    "    for solution_code in solution_codes:\n",
    "        negative_solutions = list()\n",
    "        while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "            high_score_idx = negative_code_ranking[ranking_idx]\n",
    "            \n",
    "            if high_score_idx not in solution_codes_indices:\n",
    "                negative_solutions.append(train_df[\"code\"].iloc[high_score_idx])\n",
    "            ranking_idx += 1\n",
    "        \n",
    "        for negative_solution in negative_solutions:\n",
    "            negative_pairs.append((solution_code, negative_solution))\n",
    "    \n",
    "    total_positive_pairs.extend(positive_pairs)\n",
    "    total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "pos_code1 = list(map(lambda x : x[0], total_positive_pairs))\n",
    "pos_code2 = list(map(lambda x : x[1], total_positive_pairs))\n",
    "\n",
    "neg_code1 = list(map(lambda x : x[0], total_negative_pairs))\n",
    "neg_code2 = list(map(lambda x : x[1], total_negative_pairs))\n",
    "\n",
    "pos_label = [1] * len(pos_code1)\n",
    "neg_label = [0] * len(neg_code1)\n",
    "\n",
    "pos_code1.extend(neg_code1)\n",
    "total_code1 = pos_code1\n",
    "pos_code2.extend(neg_code2)\n",
    "total_code2 = pos_code2\n",
    "pos_label.extend(neg_label)\n",
    "total_label = pos_label\n",
    "pair_data = pd.DataFrame(data={\n",
    "    \"code1\" : total_code1,\n",
    "    \"code2\" : total_code2,\n",
    "    \"similar\" : total_label\n",
    "})\n",
    "pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "pair_data.to_csv(\"./data/train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1597b05-bdef-4af0-b1d1-62dfa32e9dd7",
   "metadata": {},
   "source": [
    "#### Validation negative pair 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf8cf9d-12e1-4de7-afad-a8afe64ca4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:54<00:00,  5.51it/s]\n"
     ]
    }
   ],
   "source": [
    "codes = valid_df[\"code\"].to_list()\n",
    "problems = valid_df[\"problem_num\"].unique().tolist()\n",
    "problems.sort()\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "total_positive_pairs = list()\n",
    "total_negative_pairs = list()\n",
    "\n",
    "for problem in tqdm(problems):\n",
    "    solution_codes = valid_df[valid_df[\"problem_num\"] == problem][\"code\"]\n",
    "    positive_pairs = list(combinations(solution_codes.to_list(), 2))\n",
    "    \n",
    "    solution_codes_indices = solution_codes.index.to_list()\n",
    "    negative_pairs = list()\n",
    "    \n",
    "    first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "    negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "    negative_code_ranking = negative_code_scores.argsort()[::-1]\n",
    "    ranking_idx = 0\n",
    "    \n",
    "    for solution_code in solution_codes:\n",
    "        negative_solutions = list()\n",
    "        while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "            high_score_idx = negative_code_ranking[ranking_idx]\n",
    "            \n",
    "            if high_score_idx not in solution_codes_indices:\n",
    "                negative_solutions.append(valid_df[\"code\"].iloc[high_score_idx])\n",
    "            ranking_idx += 1\n",
    "        \n",
    "        for negative_solution in negative_solutions:\n",
    "            negative_pairs.append((solution_code, negative_solution))\n",
    "    \n",
    "    total_positive_pairs.extend(positive_pairs)\n",
    "    total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "pos_code1 = list(map(lambda x : x[0], total_positive_pairs))\n",
    "pos_code2 = list(map(lambda x : x[1], total_positive_pairs))\n",
    "\n",
    "neg_code1 = list(map(lambda x : x[0], total_negative_pairs))\n",
    "neg_code2 = list(map(lambda x : x[1], total_negative_pairs))\n",
    "\n",
    "pos_label = [1] * len(pos_code1)\n",
    "neg_label = [0] * len(neg_code1)\n",
    "\n",
    "pos_code1.extend(neg_code1)\n",
    "total_code1 = pos_code1\n",
    "pos_code2.extend(neg_code2)\n",
    "total_code2 = pos_code2\n",
    "pos_label.extend(neg_label)\n",
    "total_label = pos_label\n",
    "pair_data = pd.DataFrame(data={\n",
    "    \"code1\" : total_code1,\n",
    "    \"code2\" : total_code2,\n",
    "    \"similar\" : total_label\n",
    "})\n",
    "pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "pair_data.to_csv(\"./data/valid_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de202a2-f602-482b-a67c-e844465ce7f6",
   "metadata": {},
   "source": [
    "#### Train Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74dc87b-f2c5-4f6a-a7ca-07aa8a8406d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, RobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "# model = RobertaForSequenceClassification.from_pretrained(\"./checkpoint-4000\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "788ee5e6-7b3a-4321-b0b5-cf22d85df485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a3d399-1e6e-46e0-8594-f7750548c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f896608f-52c9-438d-974f-79026ce23268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c4fdb0db8fcc96d3\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-c4fdb0db8fcc96d3/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acfb7b79e8a4859b461bea807c6c8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ce4e0374b25f68ef\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-ce4e0374b25f68ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de97edd1eef4841bf0c6acfc5e0ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7ddc08b3f4571ab1df011c732dbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa585da63e549638255a2c6c60a8e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = \"microsoft/graphcodebert-base\"\n",
    "# MODEL = \"./checkpoint-4000\"\n",
    "# MODEL = \"huggingface/CodeBERTa-small-v1\"\n",
    "INPUT = \"./data/train_data_lv1.csv\"\n",
    "VAL_INPUT = \"./data/valid_data_lv1.csv\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=INPUT)['train']\n",
    "val_dataset = load_dataset(\"csv\", data_files=VAL_INPUT)[\"train\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding=True, max_length=MAX_LEN,truncation=True)\n",
    "    if 'similar' in examples:\n",
    "        outputs[\"labels\"] = examples[\"similar\"]\n",
    "    return outputs\n",
    "\n",
    "dataset = dataset.map(example_fn, remove_columns=['code1', 'code2', 'similar'])\n",
    "val_dataset = val_dataset.map(example_fn, remove_columns=[\"code1\", \"code2\", \"similar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75cd03a8-6862-4f86-9f7c-2dc9ba992ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_dataset['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset['attention_mask'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f84f07-1e72-4471-9497-82003b10a6ce",
   "metadata": {},
   "source": [
    "attention_mask shape -> 그냥 문장 길이수만큼 1이 들어감. 0이 없어..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f4e873f-0aa3-4761-ab1c-57ac15348b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumVal = 0\n",
    "# list(map(sum, val_dataset[\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b006dea-2a4e-4694-bfef-91fc4a87cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9f2df0-ace1-4086-95b9-9ecae4323235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator : dataset에서 뽑아온 instance들을 batch형태로 만들어 주는 역할을 수행\n",
    "# 기본적인 graphcodeBert는 수용량이 514. 그래서 256짜리 token 두개 비교하면 끝인데\n",
    "# 얘 덕에 length 512짜리 문장 두개를 비교할 수 있는거 같음\n",
    "_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "_metric = load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "def metric_fn(p):\n",
    "    preds, labels = p\n",
    "    output =  _metric.compute(references=labels, predictions=np.argmax(preds, axis=-1))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64c7fb6b-78c8-49aa-8658-43d3b4733d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    './runs/',\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=51,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #save_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    #logging_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    #metric_for_best_model= \"f1\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics= metric_fn,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=50)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be02611-122c-44d1-a147-e4d020cd6da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 600000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 28125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24001' max='28125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24001/28125 25:18:45 < 4:20:59, 0.26 it/s, Epoch 2.56/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.121941</td>\n",
       "      <td>0.956883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.110040</td>\n",
       "      <td>0.964300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.078866</td>\n",
       "      <td>0.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.085831</td>\n",
       "      <td>0.970067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.067220</td>\n",
       "      <td>0.976550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.070067</td>\n",
       "      <td>0.977133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.068004</td>\n",
       "      <td>0.977817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.062025</td>\n",
       "      <td>0.980017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.056455</td>\n",
       "      <td>0.980417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.058361</td>\n",
       "      <td>0.980033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.052606</td>\n",
       "      <td>0.983083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.070005</td>\n",
       "      <td>0.979083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.050334</td>\n",
       "      <td>0.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.053837</td>\n",
       "      <td>0.982683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.069517</td>\n",
       "      <td>0.979983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.056126</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.047536</td>\n",
       "      <td>0.986017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.055545</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.060124</td>\n",
       "      <td>0.984667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.091732</td>\n",
       "      <td>0.977083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.058806</td>\n",
       "      <td>0.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.061642</td>\n",
       "      <td>0.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.052782</td>\n",
       "      <td>0.986883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.050590</td>\n",
       "      <td>0.986700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.987550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.061623</td>\n",
       "      <td>0.985850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.986483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.049827</td>\n",
       "      <td>0.987933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.051346</td>\n",
       "      <td>0.988133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.054073</td>\n",
       "      <td>0.987083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.049070</td>\n",
       "      <td>0.988217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.988167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.048743</td>\n",
       "      <td>0.987683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.050382</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.046855</td>\n",
       "      <td>0.989217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.055114</td>\n",
       "      <td>0.987767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.988967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.049443</td>\n",
       "      <td>0.988650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.055273</td>\n",
       "      <td>0.988633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.050258</td>\n",
       "      <td>0.989150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.055843</td>\n",
       "      <td>0.988633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.988583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.067983</td>\n",
       "      <td>0.986750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.058482</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>0.989283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.054038</td>\n",
       "      <td>0.989350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.054909</td>\n",
       "      <td>0.989217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.057754</td>\n",
       "      <td>0.988883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-500\n",
      "Configuration saved in ./runs/checkpoint-500/config.json\n",
      "Model weights saved in ./runs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-1000\n",
      "Configuration saved in ./runs/checkpoint-1000/config.json\n",
      "Model weights saved in ./runs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-1000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-1500\n",
      "Configuration saved in ./runs/checkpoint-1500/config.json\n",
      "Model weights saved in ./runs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-1500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-2000\n",
      "Configuration saved in ./runs/checkpoint-2000/config.json\n",
      "Model weights saved in ./runs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-2000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-2500\n",
      "Configuration saved in ./runs/checkpoint-2500/config.json\n",
      "Model weights saved in ./runs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-2500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-3000\n",
      "Configuration saved in ./runs/checkpoint-3000/config.json\n",
      "Model weights saved in ./runs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-3000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-3500\n",
      "Configuration saved in ./runs/checkpoint-3500/config.json\n",
      "Model weights saved in ./runs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-3500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-4000\n",
      "Configuration saved in ./runs/checkpoint-4000/config.json\n",
      "Model weights saved in ./runs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-4000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-4500\n",
      "Configuration saved in ./runs/checkpoint-4500/config.json\n",
      "Model weights saved in ./runs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-4500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-5000\n",
      "Configuration saved in ./runs/checkpoint-5000/config.json\n",
      "Model weights saved in ./runs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-5000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-5500\n",
      "Configuration saved in ./runs/checkpoint-5500/config.json\n",
      "Model weights saved in ./runs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-5500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-6000\n",
      "Configuration saved in ./runs/checkpoint-6000/config.json\n",
      "Model weights saved in ./runs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-6000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-6500\n",
      "Configuration saved in ./runs/checkpoint-6500/config.json\n",
      "Model weights saved in ./runs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-6500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-7000\n",
      "Configuration saved in ./runs/checkpoint-7000/config.json\n",
      "Model weights saved in ./runs/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-7000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-7500\n",
      "Configuration saved in ./runs/checkpoint-7500/config.json\n",
      "Model weights saved in ./runs/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-7500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-8000\n",
      "Configuration saved in ./runs/checkpoint-8000/config.json\n",
      "Model weights saved in ./runs/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-8000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-8500\n",
      "Configuration saved in ./runs/checkpoint-8500/config.json\n",
      "Model weights saved in ./runs/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-8500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-9000\n",
      "Configuration saved in ./runs/checkpoint-9000/config.json\n",
      "Model weights saved in ./runs/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-9000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-9500\n",
      "Configuration saved in ./runs/checkpoint-9500/config.json\n",
      "Model weights saved in ./runs/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-9500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-10000\n",
      "Configuration saved in ./runs/checkpoint-10000/config.json\n",
      "Model weights saved in ./runs/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-10000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-10500\n",
      "Configuration saved in ./runs/checkpoint-10500/config.json\n",
      "Model weights saved in ./runs/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-10500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-11000\n",
      "Configuration saved in ./runs/checkpoint-11000/config.json\n",
      "Model weights saved in ./runs/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-11000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-11500\n",
      "Configuration saved in ./runs/checkpoint-11500/config.json\n",
      "Model weights saved in ./runs/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-11500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-12000\n",
      "Configuration saved in ./runs/checkpoint-12000/config.json\n",
      "Model weights saved in ./runs/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-12000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-12500\n",
      "Configuration saved in ./runs/checkpoint-12500/config.json\n",
      "Model weights saved in ./runs/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-12500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-13000\n",
      "Configuration saved in ./runs/checkpoint-13000/config.json\n",
      "Model weights saved in ./runs/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-13000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-13500\n",
      "Configuration saved in ./runs/checkpoint-13500/config.json\n",
      "Model weights saved in ./runs/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-13500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-14000\n",
      "Configuration saved in ./runs/checkpoint-14000/config.json\n",
      "Model weights saved in ./runs/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-14000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-14500\n",
      "Configuration saved in ./runs/checkpoint-14500/config.json\n",
      "Model weights saved in ./runs/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-14500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-15000\n",
      "Configuration saved in ./runs/checkpoint-15000/config.json\n",
      "Model weights saved in ./runs/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-15000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-15500\n",
      "Configuration saved in ./runs/checkpoint-15500/config.json\n",
      "Model weights saved in ./runs/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-15500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-16000\n",
      "Configuration saved in ./runs/checkpoint-16000/config.json\n",
      "Model weights saved in ./runs/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-16000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-16500\n",
      "Configuration saved in ./runs/checkpoint-16500/config.json\n",
      "Model weights saved in ./runs/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-16500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-17000\n",
      "Configuration saved in ./runs/checkpoint-17000/config.json\n",
      "Model weights saved in ./runs/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-17000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-17500\n",
      "Configuration saved in ./runs/checkpoint-17500/config.json\n",
      "Model weights saved in ./runs/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-17500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-18000\n",
      "Configuration saved in ./runs/checkpoint-18000/config.json\n",
      "Model weights saved in ./runs/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-18000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-18500\n",
      "Configuration saved in ./runs/checkpoint-18500/config.json\n",
      "Model weights saved in ./runs/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-18500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-19000\n",
      "Configuration saved in ./runs/checkpoint-19000/config.json\n",
      "Model weights saved in ./runs/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-19000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-19500\n",
      "Configuration saved in ./runs/checkpoint-19500/config.json\n",
      "Model weights saved in ./runs/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-19500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-20000\n",
      "Configuration saved in ./runs/checkpoint-20000/config.json\n",
      "Model weights saved in ./runs/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-20000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-20500\n",
      "Configuration saved in ./runs/checkpoint-20500/config.json\n",
      "Model weights saved in ./runs/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-20500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-21000\n",
      "Configuration saved in ./runs/checkpoint-21000/config.json\n",
      "Model weights saved in ./runs/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-21000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-21500\n",
      "Configuration saved in ./runs/checkpoint-21500/config.json\n",
      "Model weights saved in ./runs/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-21500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-22000\n",
      "Configuration saved in ./runs/checkpoint-22000/config.json\n",
      "Model weights saved in ./runs/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-22000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-22500\n",
      "Configuration saved in ./runs/checkpoint-22500/config.json\n",
      "Model weights saved in ./runs/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-22500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-23000\n",
      "Configuration saved in ./runs/checkpoint-23000/config.json\n",
      "Model weights saved in ./runs/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-23000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-23500\n",
      "Configuration saved in ./runs/checkpoint-23500/config.json\n",
      "Model weights saved in ./runs/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-23500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-24000\n",
      "Configuration saved in ./runs/checkpoint-24000/config.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:300] . unexpected pos 284400128 vs 284400016",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3212/1663367969.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_internal_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0;31m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2128\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0;31m# Push to the Hub when `save_model` is called by the user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2178\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, save_config, state_dict, save_function, push_to_hub, max_shard_size, **kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 284400128 vs 284400016"
     ]
    }
   ],
   "source": [
    "# gpu cashe clear\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883f46df-9bec-4019-8f83-6de74dc41612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./checkpoint-17500-largelv1/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/graphcodebert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./checkpoint-17500-largelv1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./checkpoint-17500-largelv1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"./checkpoint-17500-largelv1\")\n",
    "model.to(device)\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     './runs/',\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     num_train_epochs=3,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     #save_strategy=\"epoch\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     #logging_strategy=\"epoch\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     #evaluation_strategy=\"epoch\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=500,\n",
    "#     learning_rate=1e-5,\n",
    "#     #metric_for_best_model= \"f1\",\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics= metric_fn,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=20)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6d7cca2-ace1-4bfd-9b93-facbb3ad44e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e82830c223845fb2\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-e82830c223845fb2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a4961c45cf40ffacc1994402ff74a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1247fd0554ea4d4c8323cb8d0f071715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179700 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: pair_id. If pair_id are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 179700\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2808' max='2808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2808/2808 35:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TEST = \"./data/test.csv\"\n",
    "SUB = \"./data/sample_submission.csv\"\n",
    "\n",
    "test_dataset = load_dataset(\"csv\", data_files=TEST)[\"train\"]\n",
    "test_dataset = test_dataset.map(example_fn, remove_columns=[\"code1\", \"code2\"])\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "df = pd.read_csv(SUB)\n",
    "df[\"similar\"] = np.argmax(predictions.predictions, axis=-1)\n",
    "df.to_csv(\"./submissions/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7901ec6-65e5-4f15-aa81-c3438f613b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list = list(model.children())\n",
    "# print(model_list[0])\n",
    "# print(\"-\" * 100)\n",
    "# print(model_list[1])\n",
    "\n",
    "# for module in list(model.modules()):\n",
    "#     print(module)\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "for idx, params in enumerate(model.roberta.parameters()):\n",
    "    print(params.require_grad)\n",
    "    params.require_grad = False\n",
    "    print(params.require_grad)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if(idx > 10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e7843-b000-42bb-94a0-415c60f3d069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893ef96-c329-4c22-b014-1085543e6373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
