{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers datasets\n",
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (Dataset,\n",
    "                              DataLoader, \n",
    "                              RandomSampler, \n",
    "                              SequentialSampler, \n",
    "                              TensorDataset)\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import (AutoConfig, \n",
    "                          AutoTokenizer, \n",
    "                          RobertaForSequenceClassification,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorWithPadding,\n",
    "                          EarlyStoppingCallback)\n",
    "from transformers import AdamW\n",
    "from transformers import (get_scheduler, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric, load_dataset, Dataset, concatenate_datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             precision_recall_curve,\n",
    "                             f1_score,\n",
    "                             auc)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import math\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3816271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "\n",
    "  label_indices = list(range(3))\n",
    "  f1 = f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "  return {'micro f1 score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold를 위해 나누어져있는 dataset을 다시 합쳤습니다.\n",
    "train_dset = load_dataset(\"csv\", data_files=\"./train_data_lv1.csv\")['train']\n",
    "validation_dset = load_dataset(\"csv\", data_files=\"./valid_data_lv1.csv\")['train']\n",
    "rawdataset = concatenate_datasets([train_dset, validation_dset])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "# Tokenize\n",
    "def example_fn(examples):\n",
    "  outputs = tokenizer(examples['code1'], examples['code2'], padding=True, max_length=512, truncation=True)\n",
    "  if 'similar' in examples:\n",
    "      outputs[\"labels\"] = examples[\"similar\"]\n",
    "  return outputs\n",
    "dset = rawdataset.map(example_fn, remove_columns=train_dset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "  config =  AutoConfig.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "  config.num_labels = 2\n",
    "\n",
    "   \n",
    "  gap = int(len(dset) / args.k_fold)\n",
    "\n",
    "  for i in range(args.k_fold):\n",
    "        \n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\", config=config).to(device)\n",
    "        \n",
    "        print('\\n%dth Training' %(i+1))\n",
    "        \n",
    "        output_dir = args.output_dir + '_' + str(i+1)\n",
    "        logging_dir = args.logging_dir + '_' + str(i+1)\n",
    "        \n",
    "        # trainingset, validset 구성\n",
    "        total_size = len(dset)\n",
    "        total_ids = list(range(total_size))\n",
    "        del_ids = list(range(i*gap, (i+1)*gap))\n",
    "        training_ids = set(total_ids) - set(del_ids)\n",
    "        \n",
    "        training_dset = dset.select(list(training_ids))\n",
    "        eval_dset = dset.select(del_ids)\n",
    "\n",
    "        # Training Arguments -> Graphcodebert 깃허브를 참고하여 설정했습니다.\n",
    "        args.max_steps=args.epochs*len(dset)\n",
    "        args.save_steps=len(dset)//10\n",
    "        args.warmup_steps = args.max_steps//5\n",
    "        \n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=args.output_dir,                         # output directory\n",
    "          overwrite_output_dir=True,                          # overwrite output directory\n",
    "          save_total_limit=5,                                 # number of total save model.\n",
    "          save_steps=args.save_steps,                         # model saving step.\n",
    "          num_train_epochs=args.epochs,                       # total number of training epochs\n",
    "          learning_rate=args.lr,                              # learning_rate\n",
    "          per_device_train_batch_size=args.train_batch_size,  # batch size per device during training\n",
    "          per_device_eval_batch_size=args.eval_batch_size,    # batch size for evaluation\n",
    "          warmup_steps=args.warmup_steps,                     # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=args.weight_decay,                     # strength of weight decay\n",
    "          logging_dir=args.logging_dir,                       # directory for storing logs\n",
    "          logging_steps=args.logging_steps,                   # log saving step.\n",
    "          evaluation_strategy=args.evaluation_strategy,       # evaluation strategy to adopt during training\n",
    "          eval_steps=args.eval_steps,                         # evaluation step.\n",
    "          load_best_model_at_end = True, # for earlystopping\n",
    "          save_strategy = 'steps', # for earlystopping\n",
    "          logging_strategy = 'steps', # for earlystopping\n",
    "          gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        )\n",
    "\n",
    "        collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=512)\n",
    "\n",
    "        trainer = Trainer(\n",
    "          model=model,                         # the instantiated Transformers model to be trained\n",
    "          args=training_args,                  # training arguments, defined above\n",
    "          train_dataset=training_dset,            # training dataset\n",
    "          eval_dataset=eval_dset,        # evaluation dataset\n",
    "          data_collator=collator,              # collator\n",
    "          compute_metrics=compute_metrics,      # define metrics function -> micro f1\n",
    "          callbacks = [EarlyStoppingCallback(early_stopping_patience=10)],\n",
    "        )\n",
    "\n",
    "        # -- Training\n",
    "        print('Training Strats')\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb660e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'output_dir': './DACON',\n",
    "    'logging_dir': './DACON',\n",
    "    'lr': 2e-5,\n",
    "    'epochs': 3,\n",
    "    'train_batch_size': 4,\n",
    "    'weight_decay': 0.0,\n",
    "    'warmup_steps': 0,\n",
    "    'gradient_accumulation_steps':2,\n",
    "    'eval_batch_size': 8,\n",
    "    'k_fold':5,\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'save_steps': 1000,\n",
    "    'logging_steps': 1000,\n",
    "    'eval_steps':1000,\n",
    "    'max_steps':-1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e39931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
