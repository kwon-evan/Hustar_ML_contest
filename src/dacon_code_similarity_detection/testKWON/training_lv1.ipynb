{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "b06072eb-fd2a-43e1-a702-67ea305d63db",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_metric, load_dataset, load_from_disk\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "5fd5c701-e8a7-41d9-8b08-629b43101ad3",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk('./data/train_dataset_lv1')\n",
    "valid_dataset = load_from_disk('./data/valid_dataset_lv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "861f71b9-8980-4173-9674-431edd014f88",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"microsoft/graphcodebert-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "4528f711-89f7-481a-bdb5-13c55ebedab1",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "_metric = load_metric(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "5c3a4b33-874d-438e-974b-c3736fcfc259",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def metric_fn(p):\n",
    "    preds, labels = p\n",
    "    output =  _metric.compute(references=labels, predictions=np.argmax(preds, axis=-1))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "4514262a-bedb-4e3a-a3e8-9697e07999f5",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='./models/',\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    disable_tqdm = False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    optim='adamw_torch',\n",
    "    # metric_for_best_model= \"f1\",\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics= metric_fn,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "4e659920-e286-4cfa-af72-6dc46f766183",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from knockknock import discord_sender\n",
    "\n",
    "webhook_url=''\n",
    "\n",
    "@discord_sender(webhook_url=webhook_url)\n",
    "def do_train():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "219c0af9-96d3-4a56-b494-d478cd7bb144",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 300000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 28125\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11786' max='28125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11786/28125 6:26:39 < 8:56:07, 0.51 it/s, Epoch 1.26/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.195409</td>\n",
       "      <td>0.925467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.159200</td>\n",
       "      <td>0.124913</td>\n",
       "      <td>0.957133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.101645</td>\n",
       "      <td>0.965767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.103482</td>\n",
       "      <td>0.969367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.105472</td>\n",
       "      <td>0.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.135164</td>\n",
       "      <td>0.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.077462</td>\n",
       "      <td>0.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.081016</td>\n",
       "      <td>0.978167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.073270</td>\n",
       "      <td>0.979967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.065429</td>\n",
       "      <td>0.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.072735</td>\n",
       "      <td>0.979933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.062563</td>\n",
       "      <td>0.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.056764</td>\n",
       "      <td>0.983933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.056498</td>\n",
       "      <td>0.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.055388</td>\n",
       "      <td>0.984567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.078563</td>\n",
       "      <td>0.981300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>0.984733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.081810</td>\n",
       "      <td>0.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.062788</td>\n",
       "      <td>0.984667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.067495</td>\n",
       "      <td>0.985233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.061308</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.085422</td>\n",
       "      <td>0.983233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1759' max='2808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1759/2808 18:40 < 11:08, 1.57 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-500\n",
      "Configuration saved in ./models/checkpoint-500/config.json\n",
      "Model weights saved in ./models/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-1000\n",
      "Configuration saved in ./models/checkpoint-1000/config.json\n",
      "Model weights saved in ./models/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-1500\n",
      "Configuration saved in ./models/checkpoint-1500/config.json\n",
      "Model weights saved in ./models/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-2000\n",
      "Configuration saved in ./models/checkpoint-2000/config.json\n",
      "Model weights saved in ./models/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-2500\n",
      "Configuration saved in ./models/checkpoint-2500/config.json\n",
      "Model weights saved in ./models/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-3000\n",
      "Configuration saved in ./models/checkpoint-3000/config.json\n",
      "Model weights saved in ./models/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-3500\n",
      "Configuration saved in ./models/checkpoint-3500/config.json\n",
      "Model weights saved in ./models/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-4000\n",
      "Configuration saved in ./models/checkpoint-4000/config.json\n",
      "Model weights saved in ./models/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-4500\n",
      "Configuration saved in ./models/checkpoint-4500/config.json\n",
      "Model weights saved in ./models/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-5000\n",
      "Configuration saved in ./models/checkpoint-5000/config.json\n",
      "Model weights saved in ./models/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-5500\n",
      "Configuration saved in ./models/checkpoint-5500/config.json\n",
      "Model weights saved in ./models/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-6000\n",
      "Configuration saved in ./models/checkpoint-6000/config.json\n",
      "Model weights saved in ./models/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-6500\n",
      "Configuration saved in ./models/checkpoint-6500/config.json\n",
      "Model weights saved in ./models/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-4000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-7000\n",
      "Configuration saved in ./models/checkpoint-7000/config.json\n",
      "Model weights saved in ./models/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-4500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-7500\n",
      "Configuration saved in ./models/checkpoint-7500/config.json\n",
      "Model weights saved in ./models/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-5000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-8000\n",
      "Configuration saved in ./models/checkpoint-8000/config.json\n",
      "Model weights saved in ./models/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-5500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-8500\n",
      "Configuration saved in ./models/checkpoint-8500/config.json\n",
      "Model weights saved in ./models/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-6000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-9000\n",
      "Configuration saved in ./models/checkpoint-9000/config.json\n",
      "Model weights saved in ./models/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-6500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-9500\n",
      "Configuration saved in ./models/checkpoint-9500/config.json\n",
      "Model weights saved in ./models/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-7000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-10000\n",
      "Configuration saved in ./models/checkpoint-10000/config.json\n",
      "Model weights saved in ./models/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-7500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-10500\n",
      "Configuration saved in ./models/checkpoint-10500/config.json\n",
      "Model weights saved in ./models/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-8500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-11000\n",
      "Configuration saved in ./models/checkpoint-11000/config.json\n",
      "Model weights saved in ./models/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-9000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-11500\n",
      "Configuration saved in ./models/checkpoint-11500/config.json\n",
      "Model weights saved in ./models/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-9500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_17211/4283706726.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdo_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/knockknock/discord_sender.py\u001B[0m in \u001B[0;36mwrapper_sender\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m                 \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mmaster_process\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_17211/58437198.py\u001B[0m in \u001B[0;36mdo_train\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mgc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty_cache\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m     \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1319\u001B[0m             \u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m             \u001B[0mtrial\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m             \u001B[0mignore_keys_for_eval\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mignore_keys_for_eval\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1322\u001B[0m         )\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1557\u001B[0m                     \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging_nan_inf_filter\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1558\u001B[0m                     \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_torch_tpu_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1559\u001B[0;31m                     \u001B[0;32mand\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtr_loss_step\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misinf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtr_loss_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1560\u001B[0m                 ):\n\u001B[1;32m   1561\u001B[0m                     \u001B[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# SUBMISSON"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def preprocess_script(code):\n",
    "    new_code = deque()\n",
    "\n",
    "    for line in code.split('\\n'):\n",
    "        if line.lstrip().startswith('#'): # 주석으로 시작되는 행 skip\n",
    "            continue\n",
    "        line = line.rstrip()\n",
    "        if '#' in line:\n",
    "            line = line[:line.index('#')] # 주석 전까지 코드만 저장\n",
    "        line = line.replace('\\n','')      # 개행 문자를 모두 삭제함\n",
    "        line = line.replace('    ','\\t')  # 공백 4칸을 tab으로 변환\n",
    "\n",
    "        if line == '': # 전처리 후 빈 라인은 skip\n",
    "            continue\n",
    "\n",
    "        new_code.append(line)\n",
    "\n",
    "    new_code = '\\n'.join(new_code)\n",
    "    new_code = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', new_code)\n",
    "    new_code = re.sub(\"('''[\\w\\W]*?''')\", '<str>', new_code)\n",
    "    new_code = re.sub('/^(file|gopher|news|nntp|telnet|http?|https?|ftps?|sftp):\\/\\/([a-z0-9-]+\\.)+[a-z0-9]{2,4}.*$/',\n",
    "                      '<url>',\n",
    "                      new_code)\n",
    "\n",
    "    return new_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(\n",
    "        preprocess_script(examples['code1']),\n",
    "        preprocess_script(examples['code2']),\n",
    "        padding=True, max_length=MAX_LEN,truncation=True,)\n",
    "    if 'similar' in examples:\n",
    "        outputs[\"labels\"] = examples[\"similar\"]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "b2960977-35fb-4563-a6aa-a738b0f660f3",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TEST = \"./data/test.csv\"\n",
    "SUB = \"./data/sample_submission.csv\"\n",
    "\n",
    "test_dataset = load_dataset(\"csv\", data_files=TEST)[\"train\"]\n",
    "test_dataset = test_dataset.map(example_fn, remove_columns=[\"code1\", \"code2\"])\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "df = pd.read_csv(SUB)\n",
    "df[\"similar\"] = np.argmax(predictions.predictions, axis=-1)\n",
    "df.to_csv(\"./submissions/submission_preprossed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}