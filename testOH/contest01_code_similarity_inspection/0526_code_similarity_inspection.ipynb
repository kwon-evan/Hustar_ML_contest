{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdccdb56-b73a-44e9-acef-5263fc1d3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57da6d25-fa8a-4fcc-9230-945c173a8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_folder = \"./code\"\n",
    "problem_folders = os.listdir(code_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9318d9ba-5e24-4bc8-8be5-e413d09e4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [00:08<00:00, 35.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_script(script):\n",
    "    with open(script, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        preproc_lines = list()\n",
    "        for line in lines:\n",
    "            if line.lstrip().startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.rstrip()\n",
    "            if \"#\" in line:\n",
    "                line = line[:line.index(\"#\")]\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            line = line.replace(\"    \", \"\\t\")\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            preproc_lines.append(line)\n",
    "        preprocessed_script = \"\\n\".join(preproc_lines)\n",
    "    return preprocessed_script\n",
    "\n",
    "preproc_scripts = list()\n",
    "problem_nums = list()\n",
    "\n",
    "for problem_folder in tqdm(problem_folders):\n",
    "    scripts = os.listdir(os.path.join(code_folder, problem_folder))\n",
    "    problem_num = scripts[0].split(\"_\")[0]\n",
    "    for script in scripts:\n",
    "        script_file = os.path.join(code_folder, problem_folder, script)\n",
    "        preprocessed_script = preprocess_script(script_file)\n",
    "        \n",
    "        preproc_scripts.append(preprocessed_script)\n",
    "    problem_nums.extend([problem_num]*len(scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c3001f-7592-4cbc-86e3-ab40a0710fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data = {\"code\":preproc_scripts, \"problem_num\":problem_nums})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b014b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = dict()\n",
    "for code, prob_num in zip(df[\"code\"], df[\"problem_num\"]):\n",
    "    if prob_num in temp_dict:\n",
    "        temp_dict[prob_num] += 1\n",
    "    else:\n",
    "        temp_dict[prob_num] = 1\n",
    "\n",
    "# print(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c24572-eebc-4399-98a4-5bfa3aa3975a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeaf4555-712d-4799-8cac-ac3236ca1f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>160.123789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>500.930345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>108.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>97566.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                len\n",
       "count  45101.000000\n",
       "mean     160.123789\n",
       "std      500.930345\n",
       "min        5.000000\n",
       "25%       61.000000\n",
       "50%      108.000000\n",
       "75%      200.000000\n",
       "max    97566.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "df[\"tokens\"] = df[\"code\"].apply(tokenizer.tokenize)\n",
    "df[\"len\"] = df[\"tokens\"].apply(len)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ed19cf-62d0-4f99-ad4d-f435beebce4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43647.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>137.920842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.933475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>187.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>512.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                len\n",
       "count  43647.000000\n",
       "mean     137.920842\n",
       "std      104.933475\n",
       "min        5.000000\n",
       "25%       60.000000\n",
       "50%      104.000000\n",
       "75%      187.000000\n",
       "max      512.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = df[df[\"len\"] <= 512].reset_index(drop=True)\n",
    "ndf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c686693-1df2-477c-a117-89ebc043976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f28eb1e-c872-40bc-bdb3-767b4d952d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    179700.000000\n",
       "mean        392.408347\n",
       "std         923.698933\n",
       "min          20.000000\n",
       "25%         153.000000\n",
       "50%         255.000000\n",
       "75%         489.000000\n",
       "max      203699.000000\n",
       "Name: code1_len, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame[\"code1_len\"] = data_frame[\"code1\"].apply(len)\n",
    "data_frame[\"code1_len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b21656d-7b75-4d45-9b0a-b9104e962b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    179700.000000\n",
       "mean        390.010367\n",
       "std        1333.079216\n",
       "min          15.000000\n",
       "25%         146.000000\n",
       "50%         254.000000\n",
       "75%         477.000000\n",
       "max      203669.000000\n",
       "Name: code2_len, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame[\"code2_len\"] = data_frame[\"code2\"].apply(len)\n",
    "data_frame[\"code2_len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05512b08-2ae0-45f4-88a9-51a6c305e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df, train_label, valid_label = train_test_split(\n",
    "    ndf,\n",
    "    ndf[\"problem_num\"],\n",
    "    random_state=42,\n",
    "    test_size=0.1,\n",
    "    stratify=ndf[\"problem_num\"],\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805610e6-369e-4903-8dfa-481af9691617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159cc5e4-3a67-4b04-8456-bd102bae75f9",
   "metadata": {},
   "source": [
    "#### Train negative pair 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ed7de96-95fe-4249-9578-dfdad15ccf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [14:45<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "codes = train_df[\"code\"].to_list()\n",
    "problems = train_df[\"problem_num\"].unique().tolist()\n",
    "problems.sort()\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "total_positive_pairs = list()\n",
    "total_negative_pairs = list()\n",
    "\n",
    "for problem in tqdm(problems):\n",
    "    solution_codes = train_df[train_df[\"problem_num\"] == problem][\"code\"]\n",
    "    positive_pairs = list(combinations(solution_codes.to_list(), 2))\n",
    "    \n",
    "    solution_codes_indices = solution_codes.index.to_list()\n",
    "    negative_pairs = list()\n",
    "    \n",
    "    first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "    negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "    negative_code_ranking = negative_code_scores.argsort()[::-1]\n",
    "    ranking_idx = 0\n",
    "    \n",
    "    for solution_code in solution_codes:\n",
    "        negative_solutions = list()\n",
    "        while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "            high_score_idx = negative_code_ranking[ranking_idx]\n",
    "            \n",
    "            if high_score_idx not in solution_codes_indices:\n",
    "                negative_solutions.append(train_df[\"code\"].iloc[high_score_idx])\n",
    "            ranking_idx += 1\n",
    "        \n",
    "        for negative_solution in negative_solutions:\n",
    "            negative_pairs.append((solution_code, negative_solution))\n",
    "    \n",
    "    total_positive_pairs.extend(positive_pairs)\n",
    "    total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "pos_code1 = list(map(lambda x : x[0], total_positive_pairs))\n",
    "pos_code2 = list(map(lambda x : x[1], total_positive_pairs))\n",
    "\n",
    "neg_code1 = list(map(lambda x : x[0], total_negative_pairs))\n",
    "neg_code2 = list(map(lambda x : x[1], total_negative_pairs))\n",
    "\n",
    "pos_label = [1] * len(pos_code1)\n",
    "neg_label = [0] * len(neg_code1)\n",
    "\n",
    "pos_code1.extend(neg_code1)\n",
    "total_code1 = pos_code1\n",
    "pos_code2.extend(neg_code2)\n",
    "total_code2 = pos_code2\n",
    "pos_label.extend(neg_label)\n",
    "total_label = pos_label\n",
    "pair_data = pd.DataFrame(data={\n",
    "    \"code1\" : total_code1,\n",
    "    \"code2\" : total_code2,\n",
    "    \"similar\" : total_label\n",
    "})\n",
    "pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "pair_data.to_csv(\"./data/train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1597b05-bdef-4af0-b1d1-62dfa32e9dd7",
   "metadata": {},
   "source": [
    "#### Validation negative pair 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaf8cf9d-12e1-4de7-afad-a8afe64ca4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [00:55<00:00,  5.39it/s]\n"
     ]
    }
   ],
   "source": [
    "codes = valid_df[\"code\"].to_list()\n",
    "problems = valid_df[\"problem_num\"].unique().tolist()\n",
    "problems.sort()\n",
    "\n",
    "tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "total_positive_pairs = list()\n",
    "total_negative_pairs = list()\n",
    "\n",
    "for problem in tqdm(problems):\n",
    "    solution_codes = valid_df[valid_df[\"problem_num\"] == problem][\"code\"]\n",
    "    positive_pairs = list(combinations(solution_codes.to_list(), 2))\n",
    "    \n",
    "    solution_codes_indices = solution_codes.index.to_list()\n",
    "    negative_pairs = list()\n",
    "    \n",
    "    first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "    negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "    negative_code_ranking = negative_code_scores.argsort()[::-1]\n",
    "    ranking_idx = 0\n",
    "    \n",
    "    for solution_code in solution_codes:\n",
    "        negative_solutions = list()\n",
    "        while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "            high_score_idx = negative_code_ranking[ranking_idx]\n",
    "            \n",
    "            if high_score_idx not in solution_codes_indices:\n",
    "                negative_solutions.append(valid_df[\"code\"].iloc[high_score_idx])\n",
    "            ranking_idx += 1\n",
    "        \n",
    "        for negative_solution in negative_solutions:\n",
    "            negative_pairs.append((solution_code, negative_solution))\n",
    "    \n",
    "    total_positive_pairs.extend(positive_pairs)\n",
    "    total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "pos_code1 = list(map(lambda x : x[0], total_positive_pairs))\n",
    "pos_code2 = list(map(lambda x : x[1], total_positive_pairs))\n",
    "\n",
    "neg_code1 = list(map(lambda x : x[0], total_negative_pairs))\n",
    "neg_code2 = list(map(lambda x : x[1], total_negative_pairs))\n",
    "\n",
    "pos_label = [1] * len(pos_code1)\n",
    "neg_label = [0] * len(neg_code1)\n",
    "\n",
    "pos_code1.extend(neg_code1)\n",
    "total_code1 = pos_code1\n",
    "pos_code2.extend(neg_code2)\n",
    "total_code2 = pos_code2\n",
    "pos_label.extend(neg_label)\n",
    "total_label = pos_label\n",
    "pair_data = pd.DataFrame(data={\n",
    "    \"code1\" : total_code1,\n",
    "    \"code2\" : total_code2,\n",
    "    \"similar\" : total_label\n",
    "})\n",
    "pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "pair_data.to_csv(\"./data/valid_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de202a2-f602-482b-a67c-e844465ce7f6",
   "metadata": {},
   "source": [
    "#### Train Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c74dc87b-f2c5-4f6a-a7ca-07aa8a8406d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, RobertaForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ee5e6-7b3a-4321-b0b5-cf22d85df485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a3d399-1e6e-46e0-8594-f7750548c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 16:47:52.018573: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f896608f-52c9-438d-974f-79026ce23268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ce35a410e25f9cbd\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-ce35a410e25f9cbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d3875aaa184d7ca2439bd0ef3b5053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cc9a8ee57a12244a\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-cc9a8ee57a12244a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba0e53228254a79acd2e97a13006c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803de224b8d543d09c94f4aff34ec511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5133767 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d239dd577548c690fe18737496926b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59389 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = \"microsoft/graphcodebert-base\"\n",
    "INPUT = \"./data/train_data.csv\"\n",
    "VAL_INPUT = \"./data/valid_data.csv\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=INPUT)['train']\n",
    "val_dataset = load_dataset(\"csv\", data_files=VAL_INPUT)[\"train\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding=True, max_length=MAX_LEN,truncation=True)\n",
    "    if 'similar' in examples:\n",
    "        outputs[\"labels\"] = examples[\"similar\"]\n",
    "    return outputs\n",
    "\n",
    "dataset = dataset.map(example_fn, remove_columns=['code1', 'code2', 'similar'])\n",
    "val_dataset = val_dataset.map(example_fn, remove_columns=[\"code1\", \"code2\", \"similar\"])\n",
    "    \n",
    "# model = RobertaForSequenceClassification.from_pretrained(MODEL) # RobertaForSequenceClassification 는 BertForSequenceClassification 와 달리 pooler가 없는게 기본이기 때문에 문장 유사도에 사용 가능.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e9f2df0-ace1-4086-95b9-9ecae4323235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator : dataset에서 뽑아온 instance들을 batch형태로 만들어 주는 역할을 수행\n",
    "# 기본적인 graphcodeBert는 수용량이 514. 그래서 256짜리 token 두개 비교하면 끝인데\n",
    "# 얘 덕에 length 512짜리 문장 두개를 비교할 수 있는거 같음\n",
    "_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "_metric = load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "def metric_fn(p):\n",
    "    preds, labels = p\n",
    "    output =  _metric.compute(references=labels, predictions=np.argmax(preds, axis=-1))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64c7fb6b-78c8-49aa-8658-43d3b4733d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    './runs/',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #save_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    #logging_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    #metric_for_best_model= \"f1\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics= metric_fn,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=20)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7be02611-122c-44d1-a147-e4d020cd6da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5133767\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1925163\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56001' max='1925163' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  56001/1925163 27:26:23 < 915:53:57, 0.57 it/s, Epoch 0.09/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.502069</td>\n",
       "      <td>0.945495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.316342</td>\n",
       "      <td>0.950681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.319337</td>\n",
       "      <td>0.949351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.330152</td>\n",
       "      <td>0.953965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.276938</td>\n",
       "      <td>0.954470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.254684</td>\n",
       "      <td>0.956406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.258361</td>\n",
       "      <td>0.955968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.285095</td>\n",
       "      <td>0.957214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.264958</td>\n",
       "      <td>0.955345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.272171</td>\n",
       "      <td>0.953190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.254749</td>\n",
       "      <td>0.957080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.268339</td>\n",
       "      <td>0.953830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.274999</td>\n",
       "      <td>0.954167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.273175</td>\n",
       "      <td>0.955817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.277466</td>\n",
       "      <td>0.956440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.212618</td>\n",
       "      <td>0.957181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.210159</td>\n",
       "      <td>0.957164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.198267</td>\n",
       "      <td>0.958864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.187829</td>\n",
       "      <td>0.959201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.179773</td>\n",
       "      <td>0.961744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.203416</td>\n",
       "      <td>0.960632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.202838</td>\n",
       "      <td>0.962687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.111400</td>\n",
       "      <td>0.159223</td>\n",
       "      <td>0.962468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.195024</td>\n",
       "      <td>0.961222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.191440</td>\n",
       "      <td>0.961373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>0.221287</td>\n",
       "      <td>0.960683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.179181</td>\n",
       "      <td>0.965617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.198703</td>\n",
       "      <td>0.965280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.191530</td>\n",
       "      <td>0.964926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.186630</td>\n",
       "      <td>0.966290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.179268</td>\n",
       "      <td>0.963596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>0.963781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.181950</td>\n",
       "      <td>0.965095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.146687</td>\n",
       "      <td>0.966543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.232762</td>\n",
       "      <td>0.953257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.256266</td>\n",
       "      <td>0.958679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.176707</td>\n",
       "      <td>0.966778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.160429</td>\n",
       "      <td>0.969304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.167661</td>\n",
       "      <td>0.968075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.165624</td>\n",
       "      <td>0.963798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.195605</td>\n",
       "      <td>0.960363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.177887</td>\n",
       "      <td>0.966240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.164099</td>\n",
       "      <td>0.968833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.151023</td>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.172641</td>\n",
       "      <td>0.966947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.171167</td>\n",
       "      <td>0.964842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.171380</td>\n",
       "      <td>0.968765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.187733</td>\n",
       "      <td>0.966559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.163425</td>\n",
       "      <td>0.969068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.198542</td>\n",
       "      <td>0.964657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.151670</td>\n",
       "      <td>0.968294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.145513</td>\n",
       "      <td>0.969607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.970095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.161420</td>\n",
       "      <td>0.970870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.161363</td>\n",
       "      <td>0.968378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.149149</td>\n",
       "      <td>0.969658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.172520</td>\n",
       "      <td>0.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.144804</td>\n",
       "      <td>0.970281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.146099</td>\n",
       "      <td>0.971813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.154838</td>\n",
       "      <td>0.971123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.134445</td>\n",
       "      <td>0.971813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.133306</td>\n",
       "      <td>0.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.153128</td>\n",
       "      <td>0.971426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.153216</td>\n",
       "      <td>0.972806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.147517</td>\n",
       "      <td>0.972352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.169042</td>\n",
       "      <td>0.972689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.134864</td>\n",
       "      <td>0.970887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.184215</td>\n",
       "      <td>0.967081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.149137</td>\n",
       "      <td>0.970870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.165402</td>\n",
       "      <td>0.968513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.147857</td>\n",
       "      <td>0.971645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.147748</td>\n",
       "      <td>0.973328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.134424</td>\n",
       "      <td>0.970769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.154741</td>\n",
       "      <td>0.972082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.147643</td>\n",
       "      <td>0.972066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.159564</td>\n",
       "      <td>0.971645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.158442</td>\n",
       "      <td>0.972554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.157485</td>\n",
       "      <td>0.972840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.129158</td>\n",
       "      <td>0.972958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.154937</td>\n",
       "      <td>0.972992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.157464</td>\n",
       "      <td>0.972234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.133219</td>\n",
       "      <td>0.972503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.131186</td>\n",
       "      <td>0.973867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.136793</td>\n",
       "      <td>0.974423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.140074</td>\n",
       "      <td>0.973766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.131963</td>\n",
       "      <td>0.974507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.118722</td>\n",
       "      <td>0.974507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.164083</td>\n",
       "      <td>0.975147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.134864</td>\n",
       "      <td>0.974423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.163604</td>\n",
       "      <td>0.971847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.151512</td>\n",
       "      <td>0.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.975551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.151577</td>\n",
       "      <td>0.973497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.141459</td>\n",
       "      <td>0.975669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.134011</td>\n",
       "      <td>0.974238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.126302</td>\n",
       "      <td>0.974356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.126893</td>\n",
       "      <td>0.973631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.126407</td>\n",
       "      <td>0.975063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.164958</td>\n",
       "      <td>0.969691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.127887</td>\n",
       "      <td>0.974490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.167514</td>\n",
       "      <td>0.973968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.137915</td>\n",
       "      <td>0.974120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.150672</td>\n",
       "      <td>0.973564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.165848</td>\n",
       "      <td>0.971224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.124518</td>\n",
       "      <td>0.974608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.123387</td>\n",
       "      <td>0.975096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.118318</td>\n",
       "      <td>0.975231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.145187</td>\n",
       "      <td>0.974625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.139316</td>\n",
       "      <td>0.974574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.160304</td>\n",
       "      <td>0.973783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.128822</td>\n",
       "      <td>0.977555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.150088</td>\n",
       "      <td>0.974659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-500\n",
      "Configuration saved in ./runs/checkpoint-500/config.json\n",
      "Model weights saved in ./runs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-1000\n",
      "Configuration saved in ./runs/checkpoint-1000/config.json\n",
      "Model weights saved in ./runs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-1000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-1500\n",
      "Configuration saved in ./runs/checkpoint-1500/config.json\n",
      "Model weights saved in ./runs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-1500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-2000\n",
      "Configuration saved in ./runs/checkpoint-2000/config.json\n",
      "Model weights saved in ./runs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-2000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-2500\n",
      "Configuration saved in ./runs/checkpoint-2500/config.json\n",
      "Model weights saved in ./runs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-2500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-3000\n",
      "Configuration saved in ./runs/checkpoint-3000/config.json\n",
      "Model weights saved in ./runs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-3000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-3500\n",
      "Configuration saved in ./runs/checkpoint-3500/config.json\n",
      "Model weights saved in ./runs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-3500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-4000\n",
      "Configuration saved in ./runs/checkpoint-4000/config.json\n",
      "Model weights saved in ./runs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-4000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-4500\n",
      "Configuration saved in ./runs/checkpoint-4500/config.json\n",
      "Model weights saved in ./runs/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-4500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-5000\n",
      "Configuration saved in ./runs/checkpoint-5000/config.json\n",
      "Model weights saved in ./runs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-5000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-5500\n",
      "Configuration saved in ./runs/checkpoint-5500/config.json\n",
      "Model weights saved in ./runs/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-5500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-6000\n",
      "Configuration saved in ./runs/checkpoint-6000/config.json\n",
      "Model weights saved in ./runs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-6000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-6500\n",
      "Configuration saved in ./runs/checkpoint-6500/config.json\n",
      "Model weights saved in ./runs/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-6500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-7000\n",
      "Configuration saved in ./runs/checkpoint-7000/config.json\n",
      "Model weights saved in ./runs/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-7000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-7500\n",
      "Configuration saved in ./runs/checkpoint-7500/config.json\n",
      "Model weights saved in ./runs/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-7500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-8000\n",
      "Configuration saved in ./runs/checkpoint-8000/config.json\n",
      "Model weights saved in ./runs/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-8000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-8500\n",
      "Configuration saved in ./runs/checkpoint-8500/config.json\n",
      "Model weights saved in ./runs/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-8500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-9000\n",
      "Configuration saved in ./runs/checkpoint-9000/config.json\n",
      "Model weights saved in ./runs/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-9000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-9500\n",
      "Configuration saved in ./runs/checkpoint-9500/config.json\n",
      "Model weights saved in ./runs/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-9500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-10000\n",
      "Configuration saved in ./runs/checkpoint-10000/config.json\n",
      "Model weights saved in ./runs/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-10000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-10500\n",
      "Configuration saved in ./runs/checkpoint-10500/config.json\n",
      "Model weights saved in ./runs/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-10500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-11000\n",
      "Configuration saved in ./runs/checkpoint-11000/config.json\n",
      "Model weights saved in ./runs/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-11000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-11500\n",
      "Configuration saved in ./runs/checkpoint-11500/config.json\n",
      "Model weights saved in ./runs/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-11500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-12000\n",
      "Configuration saved in ./runs/checkpoint-12000/config.json\n",
      "Model weights saved in ./runs/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-12000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-12500\n",
      "Configuration saved in ./runs/checkpoint-12500/config.json\n",
      "Model weights saved in ./runs/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-12500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-13000\n",
      "Configuration saved in ./runs/checkpoint-13000/config.json\n",
      "Model weights saved in ./runs/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-13000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-13500\n",
      "Configuration saved in ./runs/checkpoint-13500/config.json\n",
      "Model weights saved in ./runs/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-13500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-14000\n",
      "Configuration saved in ./runs/checkpoint-14000/config.json\n",
      "Model weights saved in ./runs/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-14000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-14500\n",
      "Configuration saved in ./runs/checkpoint-14500/config.json\n",
      "Model weights saved in ./runs/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-14500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-15000\n",
      "Configuration saved in ./runs/checkpoint-15000/config.json\n",
      "Model weights saved in ./runs/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-15000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-15500\n",
      "Configuration saved in ./runs/checkpoint-15500/config.json\n",
      "Model weights saved in ./runs/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-15500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-16000\n",
      "Configuration saved in ./runs/checkpoint-16000/config.json\n",
      "Model weights saved in ./runs/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-16000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-16500\n",
      "Configuration saved in ./runs/checkpoint-16500/config.json\n",
      "Model weights saved in ./runs/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-16500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-17000\n",
      "Configuration saved in ./runs/checkpoint-17000/config.json\n",
      "Model weights saved in ./runs/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-17000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-17500\n",
      "Configuration saved in ./runs/checkpoint-17500/config.json\n",
      "Model weights saved in ./runs/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-17500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-18000\n",
      "Configuration saved in ./runs/checkpoint-18000/config.json\n",
      "Model weights saved in ./runs/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-18000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-18500\n",
      "Configuration saved in ./runs/checkpoint-18500/config.json\n",
      "Model weights saved in ./runs/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-18500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-19000\n",
      "Configuration saved in ./runs/checkpoint-19000/config.json\n",
      "Model weights saved in ./runs/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-19000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-19500\n",
      "Configuration saved in ./runs/checkpoint-19500/config.json\n",
      "Model weights saved in ./runs/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-19500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-20000\n",
      "Configuration saved in ./runs/checkpoint-20000/config.json\n",
      "Model weights saved in ./runs/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-20000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-20500\n",
      "Configuration saved in ./runs/checkpoint-20500/config.json\n",
      "Model weights saved in ./runs/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-20500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-21000\n",
      "Configuration saved in ./runs/checkpoint-21000/config.json\n",
      "Model weights saved in ./runs/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-21000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-21500\n",
      "Configuration saved in ./runs/checkpoint-21500/config.json\n",
      "Model weights saved in ./runs/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-21500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-22000\n",
      "Configuration saved in ./runs/checkpoint-22000/config.json\n",
      "Model weights saved in ./runs/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-22000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-22500\n",
      "Configuration saved in ./runs/checkpoint-22500/config.json\n",
      "Model weights saved in ./runs/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-22500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-23000\n",
      "Configuration saved in ./runs/checkpoint-23000/config.json\n",
      "Model weights saved in ./runs/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-23000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-23500\n",
      "Configuration saved in ./runs/checkpoint-23500/config.json\n",
      "Model weights saved in ./runs/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-23500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-24000\n",
      "Configuration saved in ./runs/checkpoint-24000/config.json\n",
      "Model weights saved in ./runs/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-24000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-24500\n",
      "Configuration saved in ./runs/checkpoint-24500/config.json\n",
      "Model weights saved in ./runs/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-24500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-25000\n",
      "Configuration saved in ./runs/checkpoint-25000/config.json\n",
      "Model weights saved in ./runs/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-25000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-25500\n",
      "Configuration saved in ./runs/checkpoint-25500/config.json\n",
      "Model weights saved in ./runs/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-25500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-26000\n",
      "Configuration saved in ./runs/checkpoint-26000/config.json\n",
      "Model weights saved in ./runs/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-26000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-26500\n",
      "Configuration saved in ./runs/checkpoint-26500/config.json\n",
      "Model weights saved in ./runs/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-26500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-27000\n",
      "Configuration saved in ./runs/checkpoint-27000/config.json\n",
      "Model weights saved in ./runs/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-27000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-27500\n",
      "Configuration saved in ./runs/checkpoint-27500/config.json\n",
      "Model weights saved in ./runs/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-27500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-28000\n",
      "Configuration saved in ./runs/checkpoint-28000/config.json\n",
      "Model weights saved in ./runs/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-28000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-28500\n",
      "Configuration saved in ./runs/checkpoint-28500/config.json\n",
      "Model weights saved in ./runs/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-28500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-29000\n",
      "Configuration saved in ./runs/checkpoint-29000/config.json\n",
      "Model weights saved in ./runs/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-29000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-29500\n",
      "Configuration saved in ./runs/checkpoint-29500/config.json\n",
      "Model weights saved in ./runs/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-29500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-30000\n",
      "Configuration saved in ./runs/checkpoint-30000/config.json\n",
      "Model weights saved in ./runs/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-30000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-30500\n",
      "Configuration saved in ./runs/checkpoint-30500/config.json\n",
      "Model weights saved in ./runs/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-30500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-31000\n",
      "Configuration saved in ./runs/checkpoint-31000/config.json\n",
      "Model weights saved in ./runs/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-31000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-31500\n",
      "Configuration saved in ./runs/checkpoint-31500/config.json\n",
      "Model weights saved in ./runs/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-31500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-32000\n",
      "Configuration saved in ./runs/checkpoint-32000/config.json\n",
      "Model weights saved in ./runs/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-32000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-32500\n",
      "Configuration saved in ./runs/checkpoint-32500/config.json\n",
      "Model weights saved in ./runs/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-32500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-33000\n",
      "Configuration saved in ./runs/checkpoint-33000/config.json\n",
      "Model weights saved in ./runs/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-33000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-33500\n",
      "Configuration saved in ./runs/checkpoint-33500/config.json\n",
      "Model weights saved in ./runs/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-33500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-34000\n",
      "Configuration saved in ./runs/checkpoint-34000/config.json\n",
      "Model weights saved in ./runs/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-34000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-34500\n",
      "Configuration saved in ./runs/checkpoint-34500/config.json\n",
      "Model weights saved in ./runs/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-34500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-35000\n",
      "Configuration saved in ./runs/checkpoint-35000/config.json\n",
      "Model weights saved in ./runs/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-35000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-35500\n",
      "Configuration saved in ./runs/checkpoint-35500/config.json\n",
      "Model weights saved in ./runs/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-35500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-36000\n",
      "Configuration saved in ./runs/checkpoint-36000/config.json\n",
      "Model weights saved in ./runs/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-36000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-36500\n",
      "Configuration saved in ./runs/checkpoint-36500/config.json\n",
      "Model weights saved in ./runs/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-36500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-37000\n",
      "Configuration saved in ./runs/checkpoint-37000/config.json\n",
      "Model weights saved in ./runs/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-37000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-37500\n",
      "Configuration saved in ./runs/checkpoint-37500/config.json\n",
      "Model weights saved in ./runs/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-37500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-38000\n",
      "Configuration saved in ./runs/checkpoint-38000/config.json\n",
      "Model weights saved in ./runs/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-38000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-38500\n",
      "Configuration saved in ./runs/checkpoint-38500/config.json\n",
      "Model weights saved in ./runs/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-38500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-39000\n",
      "Configuration saved in ./runs/checkpoint-39000/config.json\n",
      "Model weights saved in ./runs/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-39000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-39500\n",
      "Configuration saved in ./runs/checkpoint-39500/config.json\n",
      "Model weights saved in ./runs/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-39500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-40000\n",
      "Configuration saved in ./runs/checkpoint-40000/config.json\n",
      "Model weights saved in ./runs/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-40000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-40500\n",
      "Configuration saved in ./runs/checkpoint-40500/config.json\n",
      "Model weights saved in ./runs/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-40500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-41000\n",
      "Configuration saved in ./runs/checkpoint-41000/config.json\n",
      "Model weights saved in ./runs/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-41000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-41500\n",
      "Configuration saved in ./runs/checkpoint-41500/config.json\n",
      "Model weights saved in ./runs/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-41500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-42000\n",
      "Configuration saved in ./runs/checkpoint-42000/config.json\n",
      "Model weights saved in ./runs/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-42000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-42500\n",
      "Configuration saved in ./runs/checkpoint-42500/config.json\n",
      "Model weights saved in ./runs/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-42500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-43000\n",
      "Configuration saved in ./runs/checkpoint-43000/config.json\n",
      "Model weights saved in ./runs/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-43000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-43500\n",
      "Configuration saved in ./runs/checkpoint-43500/config.json\n",
      "Model weights saved in ./runs/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-43500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-44000\n",
      "Configuration saved in ./runs/checkpoint-44000/config.json\n",
      "Model weights saved in ./runs/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-44000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-44500\n",
      "Configuration saved in ./runs/checkpoint-44500/config.json\n",
      "Model weights saved in ./runs/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-44500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-45000\n",
      "Configuration saved in ./runs/checkpoint-45000/config.json\n",
      "Model weights saved in ./runs/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-45000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-45500\n",
      "Configuration saved in ./runs/checkpoint-45500/config.json\n",
      "Model weights saved in ./runs/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-45500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-46000\n",
      "Configuration saved in ./runs/checkpoint-46000/config.json\n",
      "Model weights saved in ./runs/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-46000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-46500\n",
      "Configuration saved in ./runs/checkpoint-46500/config.json\n",
      "Model weights saved in ./runs/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-46500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-47000\n",
      "Configuration saved in ./runs/checkpoint-47000/config.json\n",
      "Model weights saved in ./runs/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-47000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-47500\n",
      "Configuration saved in ./runs/checkpoint-47500/config.json\n",
      "Model weights saved in ./runs/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-47500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-48000\n",
      "Configuration saved in ./runs/checkpoint-48000/config.json\n",
      "Model weights saved in ./runs/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-48000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-48500\n",
      "Configuration saved in ./runs/checkpoint-48500/config.json\n",
      "Model weights saved in ./runs/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-48500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-49000\n",
      "Configuration saved in ./runs/checkpoint-49000/config.json\n",
      "Model weights saved in ./runs/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-49000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-49500\n",
      "Configuration saved in ./runs/checkpoint-49500/config.json\n",
      "Model weights saved in ./runs/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-49500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-50000\n",
      "Configuration saved in ./runs/checkpoint-50000/config.json\n",
      "Model weights saved in ./runs/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-50000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-50500\n",
      "Configuration saved in ./runs/checkpoint-50500/config.json\n",
      "Model weights saved in ./runs/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-50500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-51000\n",
      "Configuration saved in ./runs/checkpoint-51000/config.json\n",
      "Model weights saved in ./runs/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-51000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-51500\n",
      "Configuration saved in ./runs/checkpoint-51500/config.json\n",
      "Model weights saved in ./runs/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-51500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-52000\n",
      "Configuration saved in ./runs/checkpoint-52000/config.json\n",
      "Model weights saved in ./runs/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-52000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-52500\n",
      "Configuration saved in ./runs/checkpoint-52500/config.json\n",
      "Model weights saved in ./runs/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-52500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-53000\n",
      "Configuration saved in ./runs/checkpoint-53000/config.json\n",
      "Model weights saved in ./runs/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-53000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-53500\n",
      "Configuration saved in ./runs/checkpoint-53500/config.json\n",
      "Model weights saved in ./runs/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-53500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-54000\n",
      "Configuration saved in ./runs/checkpoint-54000/config.json\n",
      "Model weights saved in ./runs/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-54000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-54500\n",
      "Configuration saved in ./runs/checkpoint-54500/config.json\n",
      "Model weights saved in ./runs/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-54500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-55000\n",
      "Configuration saved in ./runs/checkpoint-55000/config.json\n",
      "Model weights saved in ./runs/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-55000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-55500\n",
      "Configuration saved in ./runs/checkpoint-55500/config.json\n",
      "Model weights saved in ./runs/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-55500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/iml/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 59389\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./runs/checkpoint-56000\n",
      "Configuration saved in ./runs/checkpoint-56000/config.json\n",
      "Model weights saved in ./runs/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in ./runs/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in ./runs/checkpoint-56000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:300] . unexpected pos 678533440 vs 678533328",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3871/1663367969.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 678533440 vs 678533328"
     ]
    }
   ],
   "source": [
    "# gpu cashe clear\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "883f46df-9bec-4019-8f83-6de74dc41612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./runs/checkpoint-53500/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/graphcodebert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./runs/checkpoint-53500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./runs/checkpoint-53500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"./runs/checkpoint-53500\")\n",
    "model.to(device)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    './runs/',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #save_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    #logging_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    #metric_for_best_model= \"f1\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=_collator,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics= metric_fn,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=20)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6d7cca2-ace1-4bfd-9b93-facbb3ad44e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e82830c223845fb2\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-e82830c223845fb2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b7237015cf4366a2cce4da234efdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f788909969ad4c8ca1de01c2991f03df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179700 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: pair_id. If pair_id are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 179700\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2808' max='2808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2808/2808 34:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TEST = \"./data/test.csv\"\n",
    "SUB = \"./data/sample_submission.csv\"\n",
    "\n",
    "test_dataset = load_dataset(\"csv\", data_files=TEST)[\"train\"]\n",
    "test_dataset = test_dataset.map(example_fn, remove_columns=[\"code1\", \"code2\"])\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "df = pd.read_csv(SUB)\n",
    "df[\"similar\"] = np.argmax(predictions.predictions, axis=-1)\n",
    "df.to_csv(\"./submissions/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7901ec6-65e5-4f15-aa81-c3438f613b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RobertaClassificationHead(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_list = list(model.children())\n",
    "print(model_list[0])\n",
    "print(\"-\" * 100)\n",
    "print(model_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e7843-b000-42bb-94a0-415c60f3d069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
