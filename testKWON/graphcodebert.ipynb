{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e721e79",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IMPORT & SET LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a57343e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (TrainingArguments,\n",
    "                          Trainer,\n",
    "                          EarlyStoppingCallback,\n",
    "                          DataCollatorWithPadding,\n",
    "                          RobertaConfig,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          RobertaTokenizer)\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "from model import Model\n",
    "from parser import DFG_python\n",
    "from parser import ( tree_to_token_index,\n",
    "                     index_to_code_token,\n",
    "                     tree_to_variable_index )\n",
    "from tree_sitter import Language, Parser\n",
    "from parser import TextDataset\n",
    "from datasets import load_metric\n",
    "from transformers import DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import re\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers[0].stream = sys.stdout\n",
    "\n",
    "LANGUAGE = Language('./clonedetection/parser/my-languages.so', 'python')\n",
    "_parser = Parser()\n",
    "_parser.set_language(LANGUAGE)\n",
    "parser = [_parser, DFG_python]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da993aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SET MODEL, CONFIG, TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e3e608",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"microsoft/graphcodebert-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = RobertaConfig.from_pretrained(MODEL)\n",
    "config.num_labels=1\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.truncation_side = 'left'\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, config=config)\n",
    "model = Model(model, tokenizer=tokenizer, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45740c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MAKE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import re\n",
    "# PREPROCESSING FOR CODE SCRIPT\n",
    "def preprocess_script(df:pd.DataFrame):\n",
    "    new_codes={'code1': deque(), 'code2': deque()}\n",
    "    codes = (df.code1, df.code2)\n",
    "\n",
    "    for i, code in enumerate(codes, start=1):     # loop code1, code2\n",
    "        for c in tqdm(code):                            # loop code series\n",
    "            new_code = deque()\n",
    "            for line in c.split('\\n'):\n",
    "                if line.lstrip().startswith('#'): # 주석으로 시작되는 행 skip\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                if '#' in line:\n",
    "                    line = line[:line.index('#')] # 주석 전까지 코드만 저장\n",
    "                line = line.replace('\\n','')      # 개행 문자를 모두 삭제함\n",
    "                line = line.replace('    ','\\t')  # 공백 4칸을 tab으로 변환\n",
    "\n",
    "                if line == '': # 전처리 후 빈 라인은 skip\n",
    "                    continue\n",
    "                \n",
    "                new_code.append(line)\n",
    "                \n",
    "            new_code = '\\n'.join(new_code)\n",
    "            new_code = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', new_code)\n",
    "            new_code = re.sub(\"('''[\\w\\W]*?''')\", '<str>', new_code)\n",
    "            new_code = re.sub('/^(file|gopher|news|nntp|telnet|https?|ftps?|sftp):\\/\\/([a-z0-9-]+\\.)+[a-z0-9]{2,4}.*$/',\n",
    "                              '<url>',\n",
    "                              new_code)\n",
    "            \n",
    "            new_codes[f'code{i}'].append(new_code)\n",
    "            \n",
    "    return pd.DataFrame(data={'code1':new_codes['code1'], 'code2':new_codes['code2'], 'similar':df.similar})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data_lv1.csv')\n",
    "preprocess_script(train_data).to_csv('data/train_data.csv', mode='w', index=False)\n",
    "valid_data = pd.read_csv('data/valid_data_lv1.csv')\n",
    "preprocess_script(valid_data).to_csv('data/valid_data.csv', mode='w', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(\n",
    "    file_path='data/train_data.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    parser=parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_dataset = TextDataset(\n",
    "    file_path='data/valid_data.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    parser=parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FUNCTIONS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(valid_dataset, model, eval_batch_size, eval_when_training=False):\n",
    "    #build dataloader\n",
    "    eval_sampler = SequentialSampler(valid_dataset)\n",
    "    eval_dataloader = DataLoader(valid_dataset, sampler=eval_sampler,batch_size=eval_batch_size,num_workers=4)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if torch.cuda.device_count() > 1 and eval_when_training is False:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(valid_dataset))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    logits=[]\n",
    "    y_trues=[]\n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        (inputs_ids_1,position_idx_1,attn_mask_1,\n",
    "        inputs_ids_2,position_idx_2,attn_mask_2,\n",
    "        labels)=[x.to(device)  for x in batch]\n",
    "        with torch.no_grad():\n",
    "            lm_loss,logit = model(inputs_ids_1,position_idx_1,attn_mask_1,inputs_ids_2,position_idx_2,attn_mask_2,labels)\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "            logits.append(logit.cpu().numpy())\n",
    "            y_trues.append(labels.cpu().numpy())\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    #calculate scores\n",
    "    logits=np.concatenate(logits,0)\n",
    "    y_trues=np.concatenate(y_trues,0)\n",
    "    best_threshold=0.5\n",
    "    best_f1=0\n",
    "\n",
    "    y_preds=logits[:,1]>best_threshold\n",
    "    \n",
    "    from sklearn.metrics import recall_score\n",
    "    recall=recall_score(y_trues, y_preds)\n",
    "    from sklearn.metrics import precision_score\n",
    "    precision=precision_score(y_trues, y_preds)\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1=f1_score(y_trues, y_preds)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy=accuracy_score(y_trues, y_preds)\n",
    "    result = {\n",
    "        \"eval_recall\": float(recall),\n",
    "        \"eval_precision\": float(precision),\n",
    "        \"eval_f1\": float(f1),\n",
    "        \"eval_threshold\":best_threshold,\n",
    "        \"eval_accuracy\":float(accuracy)\n",
    "    }\n",
    "\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(round(result[key],4)))\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from knockknock import discord_sender\n",
    "\n",
    "webhook_url=''\n",
    "\n",
    "@discord_sender(webhook_url=webhook_url)\n",
    "def train(model,\n",
    "          train_dataset:TextDataset,\n",
    "          valid_dataset:TextDataset,\n",
    "          train_batch_size=4,\n",
    "          eval_batch_size=8,\n",
    "          epochs=1,\n",
    "          weight_decay=0.0,\n",
    "          learning_rate=2e-5,\n",
    "          adam_epsilon=1e-8,\n",
    "          gradient_accumulation_steps=4,\n",
    "          max_grad_norm=1.0,\n",
    "          output_dir='./models/'\n",
    "          ):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "\n",
    "    #build dataloader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size, num_workers=4)\n",
    "\n",
    "    max_steps=epochs*len(train_dataloader)\n",
    "    save_steps=len(train_dataloader)//10\n",
    "    warmup_steps=max_steps//5\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=max_steps)\n",
    "\n",
    "    # multi-gpu training\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", train_batch_size//max(torch.cuda.device_count(), 1))\n",
    "    logger.info(\"  Total train batch size = %d\",train_batch_size*gradient_accumulation_steps)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", max_steps)\n",
    "\n",
    "    global_step=0\n",
    "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
    "    best_f1=0\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for idx in range(epochs):\n",
    "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "        tr_num=0\n",
    "        train_loss=0\n",
    "        for step, batch in enumerate(bar):\n",
    "            (inputs_ids_1,position_idx_1,attn_mask_1,\n",
    "            inputs_ids_2,position_idx_2,attn_mask_2,\n",
    "            labels)=[x.to(device)  for x in batch]\n",
    "            model.train()\n",
    "            loss,logits = model(inputs_ids_1,position_idx_1,attn_mask_1,inputs_ids_2,position_idx_2,attn_mask_2,labels)\n",
    "\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_num+=1\n",
    "            train_loss+=loss.item()\n",
    "            if avg_loss==0:\n",
    "                avg_loss=tr_loss\n",
    "\n",
    "            avg_loss=round(train_loss/tr_num,5)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(idx,avg_loss))\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "                output_flag=True\n",
    "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
    "\n",
    "                if global_step % save_steps == 0:\n",
    "                    results = evaluate(valid_dataset, model, eval_batch_size)\n",
    "\n",
    "                    # Save model checkpoint\n",
    "                    if results['eval_f1']>best_f1:\n",
    "                        best_f1=results['eval_f1']\n",
    "                        logger.info(\"  \"+\"*\"*20)\n",
    "                        logger.info(\"  Best f1:%s\",round(best_f1,4))\n",
    "                        logger.info(\"  \"+\"*\"*20)\n",
    "\n",
    "                        checkpoint_prefix = 'checkpoint-best-f1'\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format(checkpoint_prefix))\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "                        model_to_save = model.module if hasattr(model,'module') else model\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
    "                        torch.save(model_to_save.state_dict(), output_dir)\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(test_dataset: TextDataset,\n",
    "         model,\n",
    "         eval_batch_size=16,\n",
    "         best_threshold=0):\n",
    "    #build dataloader\n",
    "    eval_sampler = SequentialSampler(test_dataset)\n",
    "    eval_dataloader = DataLoader(test_dataset, sampler=eval_sampler, batch_size=eval_batch_size,num_workers=4)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running Test *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_dataset))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    logits=[]\n",
    "    y_trues=[]\n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        (inputs_ids_1,position_idx_1,attn_mask_1,\n",
    "        inputs_ids_2,position_idx_2,attn_mask_2,\n",
    "        labels)=[x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            lm_loss,logit = model(inputs_ids_1,position_idx_1,attn_mask_1,inputs_ids_2,position_idx_2,attn_mask_2,labels)\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "            logits.append(logit.cpu().numpy())\n",
    "            y_trues.append(labels.cpu().numpy())\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    #output result\n",
    "    logits=np.concatenate(logits,0)\n",
    "    y_preds=logits[:,1]>best_threshold\n",
    "\n",
    "    return y_preds*1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DO TRAIN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model,\n",
    "      train_dataset=train_dataset,\n",
    "      valid_dataset=valid_dataset,\n",
    "      train_batch_size=2,\n",
    "      epochs=3,\n",
    "      weight_decay=0.001\n",
    "     )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MAKE SUBMISSION.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import re\n",
    "# PREPROCESSING FOR CODE SCRIPT\n",
    "def preprocess_script(df:pd.DataFrame):\n",
    "    new_codes={'code1': deque(), 'code2': deque()}\n",
    "    codes = (df.code1, df.code2)\n",
    "\n",
    "    for i, code in enumerate(codes, start=1):     # loop code1, code2\n",
    "        for c in tqdm(code):                            # loop code series\n",
    "            new_code = deque()\n",
    "            for line in c.split('\\n'):\n",
    "                if line.lstrip().startswith('#'): # 주석으로 시작되는 행 skip\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                if '#' in line:\n",
    "                    line = line[:line.index('#')] # 주석 전까지 코드만 저장\n",
    "                line = line.replace('\\n','')      # 개행 문자를 모두 삭제함\n",
    "                line = line.replace('    ','\\t')  # 공백 4칸을 tab으로 변환\n",
    "\n",
    "                if line == '': # 전처리 후 빈 라인은 skip\n",
    "                    continue\n",
    "                \n",
    "                new_code.append(line)\n",
    "                \n",
    "            new_code = '\\n'.join(new_code)\n",
    "            new_code = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', new_code)\n",
    "            new_code = re.sub(\"('''[\\w\\W]*?''')\", '<str>', new_code)\n",
    "            new_code = re.sub('/^(file|gopher|news|nntp|telnet|https?|ftps?|sftp):\\/\\/([a-z0-9-]+\\.)+[a-z0-9]{2,4}.*$/',\n",
    "                              '<url>',\n",
    "                              new_code)\n",
    "            \n",
    "            new_codes[f'code{i}'].append(new_code)\n",
    "            \n",
    "    return pd.DataFrame(data={'code1':new_codes['code1'], 'code2':new_codes['code2'], 'similar':df.similar})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_data = test_data.drop('pair_id', axis=1)\n",
    "test_data['similar'] = [None] * len(test_data)\n",
    "test_data = preprocess_script(test_data)\n",
    "test_data.to_csv('data/test_1.csv', mode='w', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(\n",
    "    file_path='data/test_1.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    parser=parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "sub['similar'] = test(test_dataset, model, eval_batch_size=16, best_threshold=0.5)\n",
    "sub.to_csv('submissions/graphcodebert_submission.csv', mode='w', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0d93a05-4950-4ac9-aa77-129adeba2dd9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 179700/179700 [15:49<00:00, 189.18it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TextDataset(\n",
    "    file_path='data/test_1.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    parser=parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99e50a23",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running Test *****\n",
      "  Num examples = 179700\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/11232 [00:00<?, ?it/s]/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|███████████████████████████████████| 11232/11232 [1:56:44<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "sub['similar'] = test(test_dataset, model, eval_batch_size=16, best_threshold=0.5)\n",
    "sub.to_csv('submissions/graphcodebert_submission.csv', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b19f895e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179695</th>\n",
       "      <td>179696</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179696</th>\n",
       "      <td>179697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179697</th>\n",
       "      <td>179698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179698</th>\n",
       "      <td>179699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179699</th>\n",
       "      <td>179700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pair_id  similar\n",
       "0             1        1\n",
       "1             2        1\n",
       "2             3        1\n",
       "3             4        1\n",
       "4             5        1\n",
       "...         ...      ...\n",
       "179695   179696        1\n",
       "179696   179697        1\n",
       "179697   179698        1\n",
       "179698   179699        1\n",
       "179699   179700        1\n",
       "\n",
       "[179700 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee5bf2-4b3e-46da-b38d-2ae68f1db613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}