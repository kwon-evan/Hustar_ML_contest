{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (TrainingArguments,\n",
    "                          Trainer,\n",
    "                          EarlyStoppingCallback,\n",
    "                          DataCollatorWithPadding,\n",
    "                          RobertaConfig,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          RobertaTokenizer)\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "from model import Model\n",
    "from parser import DFG_python\n",
    "from parser import ( tree_to_token_index,\n",
    "                     index_to_code_token,\n",
    "                     tree_to_variable_index )\n",
    "from tree_sitter import Language, Parser\n",
    "from parser import TextDataset\n",
    "from datasets import load_metric\n",
    "from transformers import DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import re\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers[0].stream = sys.stdout\n",
    "\n",
    "LANGUAGE = Language('./clonedetection/parser/my-languages.so', 'python')\n",
    "_parser = Parser()\n",
    "_parser.set_language(LANGUAGE)\n",
    "parser = [_parser, DFG_python]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"microsoft/graphcodebert-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = RobertaConfig.from_pretrained(MODEL)\n",
    "config.num_labels=1\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.truncation_side = 'left'\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, config=config)\n",
    "model = Model(model, tokenizer=tokenizer, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [06:42<00:00, 745.25it/s]   \n",
      "100%|██████████| 30000/30000 [00:44<00:00, 681.75it/s]  \n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    file_path='data/train_data_lv1.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    parser=parser\n",
    ")\n",
    "valid_dataset = TextDataset(\n",
    "    file_path='data/valid_data_lv1.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    parser=parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(valid_dataset, model, eval_batch_size, eval_when_training=False):\n",
    "    #build dataloader\n",
    "    eval_sampler = SequentialSampler(valid_dataset)\n",
    "    eval_dataloader = DataLoader(valid_dataset, sampler=eval_sampler,batch_size=eval_batch_size,num_workers=4)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if torch.cuda.device_count() > 1 and eval_when_training is False:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(valid_dataset))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    logits=[]\n",
    "    y_trues=[]\n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        (inputs_ids_1,position_idx_1,attn_mask_1,\n",
    "        inputs_ids_2,position_idx_2,attn_mask_2,\n",
    "        labels)=[x.to(device)  for x in batch]\n",
    "        with torch.no_grad():\n",
    "            lm_loss,logit = model(inputs_ids_1,position_idx_1,attn_mask_1,inputs_ids_2,position_idx_2,attn_mask_2,labels)\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "            logits.append(logit.cpu().numpy())\n",
    "            y_trues.append(labels.cpu().numpy())\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    #calculate scores\n",
    "    logits=np.concatenate(logits,0)\n",
    "    y_trues=np.concatenate(y_trues,0)\n",
    "    best_threshold=0.5\n",
    "    best_f1=0\n",
    "\n",
    "    y_preds=logits[:,1]>best_threshold\n",
    "    from sklearn.metrics import recall_score\n",
    "    recall=recall_score(y_trues, y_preds)\n",
    "    from sklearn.metrics import precision_score\n",
    "    precision=precision_score(y_trues, y_preds)\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1=f1_score(y_trues, y_preds)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy=accuracy_score(y_trues, y_preds)\n",
    "    result = {\n",
    "        \"eval_recall\": float(recall),\n",
    "        \"eval_precision\": float(precision),\n",
    "        \"eval_f1\": float(f1),\n",
    "        \"eval_threshold\":best_threshold,\n",
    "        \"eval_accuracy\":float(accuracy)\n",
    "    }\n",
    "\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(round(result[key],4)))\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from knockknock import discord_sender\n",
    "\n",
    "webhook_url='https://discord.com/api/webhooks/982207199617093692/h6rKBojabzCvt-RKaPatryR737apzScuIgSMYC6MFey4wgX-gUo2ZdxeMcAR9x9I5qOP'\n",
    "\n",
    "@discord_sender(webhook_url=webhook_url)\n",
    "def train(model,\n",
    "          train_dataset:TextDataset,\n",
    "          valid_dataset:TextDataset,\n",
    "          train_batch_size=4,\n",
    "          eval_batch_size=8,\n",
    "          epochs=1,\n",
    "          weight_decay=0.0,\n",
    "          learning_rate=2e-5,\n",
    "          adam_epsilon=1e-8,\n",
    "          gradient_accumulation_steps=4,\n",
    "          max_grad_norm=1.0,\n",
    "          output_dir='./models/'\n",
    "          ):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "\n",
    "    #build dataloader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size, num_workers=4)\n",
    "\n",
    "    max_steps=epochs*len(train_dataloader)\n",
    "    save_steps=len(train_dataloader)//10\n",
    "    warmup_steps=max_steps//5\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=max_steps)\n",
    "\n",
    "    # multi-gpu training\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", train_batch_size//max(torch.cuda.device_count(), 1))\n",
    "    logger.info(\"  Total train batch size = %d\",train_batch_size*gradient_accumulation_steps)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", max_steps)\n",
    "\n",
    "    global_step=0\n",
    "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
    "    best_f1=0\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for idx in range(epochs):\n",
    "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "        tr_num=0\n",
    "        train_loss=0\n",
    "        for step, batch in enumerate(bar):\n",
    "            (inputs_ids_1,position_idx_1,attn_mask_1,\n",
    "            inputs_ids_2,position_idx_2,attn_mask_2,\n",
    "            labels)=[x.to(device)  for x in batch]\n",
    "            model.train()\n",
    "            loss,logits = model(inputs_ids_1,position_idx_1,attn_mask_1,inputs_ids_2,position_idx_2,attn_mask_2,labels)\n",
    "\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_num+=1\n",
    "            train_loss+=loss.item()\n",
    "            if avg_loss==0:\n",
    "                avg_loss=tr_loss\n",
    "\n",
    "            avg_loss=round(train_loss/tr_num,5)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(idx,avg_loss))\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "                output_flag=True\n",
    "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
    "\n",
    "                if global_step % save_steps == 0:\n",
    "                    results = evaluate(valid_dataset, model, eval_batch_size)\n",
    "\n",
    "                    # Save model checkpoint\n",
    "                    if results['eval_f1']>best_f1:\n",
    "                        best_f1=results['eval_f1']\n",
    "                        logger.info(\"  \"+\"*\"*20)\n",
    "                        logger.info(\"  Best f1:%s\",round(best_f1,4))\n",
    "                        logger.info(\"  \"+\"*\"*20)\n",
    "\n",
    "                        checkpoint_prefix = 'checkpoint-best-f1'\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format(checkpoint_prefix))\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "                        model_to_save = model.module if hasattr(model,'module') else model\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
    "                        torch.save(model_to_save.state_dict(), output_dir)\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def test(test_dataset: TextDataset,\n",
    "         model,\n",
    "         eval_batch_size=16,\n",
    "         best_threshold=0):\n",
    "    #build dataloader\n",
    "    eval_sampler = SequentialSampler(test_dataset)\n",
    "    eval_dataloader = DataLoader(test_dataset, sampler=eval_sampler, batch_size=eval_batch_size,num_workers=4)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running Test *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_dataset))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    logits=[]\n",
    "    y_trues=[]\n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        (inputs_ids_1,position_idx_1,attn_mask_1,\n",
    "        inputs_ids_2,position_idx_2,attn_mask_2,\n",
    "        labels)=[x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            lm_loss,logit = model(inputs_ids_1,position_idx_1,attn_mask_1,inputs_ids_2,position_idx_2,attn_mask_2,labels)\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "            logits.append(logit.cpu().numpy())\n",
    "            y_trues.append(labels.cpu().numpy())\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    #output result\n",
    "    logits=np.concatenate(logits,0)\n",
    "    y_preds=logits[:,1]>best_threshold\n",
    "\n",
    "    return [1 if p == True else 0 for p in y_preds]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model, train_dataset=train_dataset, valid_dataset=valid_dataset, train_batch_size=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import re\n",
    "# PREPROCESSING FOR CODE SCRIPT\n",
    "def preprocess_script(df:pd.DataFrame):\n",
    "    new_code1=deque()\n",
    "    new_code2=deque()\n",
    "    code1 = df.code1\n",
    "    code2 = df.code2\n",
    "\n",
    "    for code in code1:\n",
    "        new_code = deque()\n",
    "        for line in code.split('\\n'):\n",
    "            if line.lstrip().startswith('#'): # 주석으로 시작되는 행 skip\n",
    "                continue\n",
    "            if \"http\" in line:\n",
    "                continue\n",
    "\n",
    "            line = line.rstrip()\n",
    "            if '#' in line:\n",
    "                line = line[:line.index('#')] # 주석 전까지 코드만 저장\n",
    "            line = line.replace('\\n','') # 개행 문자를 모두 삭제함\n",
    "            line = line.replace('    ','\\t') # 공백 4칸을 tab으로 변환\n",
    "\n",
    "            if line == '': # 전처리 후 빈 라인은 skip\n",
    "                continue\n",
    "\n",
    "            new_code.append(line)\n",
    "        new_code = '\\n'.join(new_code)\n",
    "        new_code = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', new_code)\n",
    "        new_code = re.sub(\"('''[\\w\\W]*?''')\", '<str>', new_code)\n",
    "        new_code1.append(new_code)\n",
    "\n",
    "    for code in code2:\n",
    "        new_code = deque()\n",
    "        for line in code.split('\\n'):\n",
    "            if line.lstrip().startswith('#'): # 주석으로 시작되는 행 skip\n",
    "                continue\n",
    "            if \"http\" in line:\n",
    "                continue\n",
    "\n",
    "            line = line.rstrip()\n",
    "            if '#' in line:\n",
    "                line = line[:line.index('#')] # 주석 전까지 코드만 저장\n",
    "            line = line.replace('\\n','') # 개행 문자를 모두 삭제함\n",
    "            line = line.replace('    ','\\t') # 공백 4칸을 tab으로 변환\n",
    "\n",
    "            if line == '': # 전처리 후 빈 라인은 skip\n",
    "                continue\n",
    "\n",
    "            new_code.append(line)\n",
    "        new_code = '\\n'.join(new_code)\n",
    "        new_code = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', new_code)\n",
    "        new_code = re.sub(\"('''[\\w\\W]*?''')\", '<str>', new_code)\n",
    "        new_code2.append(new_code)\n",
    "\n",
    "    return pd.DataFrame(data={'code1':new_code1, 'code2':new_code2, 'similar':df.similar})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179700/179700 [15:49<00:00, 189.20it/s] \n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_data = test_data.drop('pair_id', axis=1)\n",
    "test_data['similar'] = [0] * len(test_data)\n",
    "test_data = preprocess_script(test_data)\n",
    "test_data.to_csv('data/test_1.csv', mode='w', index=False)\n",
    "test_dataset = TextDataset(\n",
    "    file_path='data/test_1.csv',\n",
    "    tokenizer=tokenizer,\n",
    "    parser=parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "sub['similar'] = test(test_dataset, model, eval_batch_size=16, best_threshold=0.5)\n",
    "sub.to_csv('submissions/graphcodebert_submission.csv', mode='w', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "        pair_id  similar\n0             1        1\n1             2        0\n2             3        0\n3             4        0\n4             5        0\n...         ...      ...\n179695   179696        1\n179696   179697        0\n179697   179698        1\n179698   179699        0\n179699   179700        1\n\n[179700 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pair_id</th>\n      <th>similar</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>179695</th>\n      <td>179696</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>179696</th>\n      <td>179697</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>179697</th>\n      <td>179698</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>179698</th>\n      <td>179699</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>179699</th>\n      <td>179700</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>179700 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}