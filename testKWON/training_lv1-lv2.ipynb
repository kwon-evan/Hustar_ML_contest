{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "b06072eb-fd2a-43e1-a702-67ea305d63db",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_metric, load_dataset, load_from_disk\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "5fd5c701-e8a7-41d9-8b08-629b43101ad3",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk('./data/train_dataset_lv2')\n",
    "valid_dataset = load_from_disk('./data/valid_dataset_lv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "861f71b9-8980-4173-9674-431edd014f88",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = \"microsoft/graphcodebert-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./models/checkpoint-8000\")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "4528f711-89f7-481a-bdb5-13c55ebedab1",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "_metric = load_metric(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "5c3a4b33-874d-438e-974b-c3736fcfc259",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def metric_fn(p):\n",
    "    preds, labels = p\n",
    "    output =  _metric.compute(references=labels, predictions=np.argmax(preds, axis=-1))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "4514262a-bedb-4e3a-a3e8-9697e07999f5",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='./models/',\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    disable_tqdm = False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    optim='adamw_torch',\n",
    "    # metric_for_best_model= \"f1\",\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics= metric_fn,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "4e659920-e286-4cfa-af72-6dc46f766183",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from knockknock import discord_sender\n",
    "\n",
    "webhook_url='https://discord.com/api/webhooks/981021972697858078/cKpZXsyxyFGptLsMiFfWdEbjwavkO0qgkgWGW3fyYeBxMkJFebDq9U5M4vgDibgM3Ew6'\n",
    "\n",
    "@discord_sender(webhook_url=webhook_url)\n",
    "def do_train():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "219c0af9-96d3-4a56-b494-d478cd7bb144",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 90000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 8436\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5500' max='8436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5500/8436 2:23:55 < 1:16:51, 0.64 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>2.927912</td>\n",
       "      <td>0.504889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>4.304398</td>\n",
       "      <td>0.517667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>4.695354</td>\n",
       "      <td>0.509889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>4.952650</td>\n",
       "      <td>0.492667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>4.749534</td>\n",
       "      <td>0.496556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>4.936773</td>\n",
       "      <td>0.495111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>5.043801</td>\n",
       "      <td>0.503667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>5.294828</td>\n",
       "      <td>0.497444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>5.073262</td>\n",
       "      <td>0.504333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>5.143164</td>\n",
       "      <td>0.498667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>5.348756</td>\n",
       "      <td>0.497556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-500\n",
      "Configuration saved in ./models/checkpoint-500/config.json\n",
      "Model weights saved in ./models/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-1000\n",
      "Configuration saved in ./models/checkpoint-1000/config.json\n",
      "Model weights saved in ./models/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-1000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-1500\n",
      "Configuration saved in ./models/checkpoint-1500/config.json\n",
      "Model weights saved in ./models/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-1500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-2000\n",
      "Configuration saved in ./models/checkpoint-2000/config.json\n",
      "Model weights saved in ./models/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-2000/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-2500\n",
      "Configuration saved in ./models/checkpoint-2500/config.json\n",
      "Model weights saved in ./models/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-2500/special_tokens_map.json\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-3000\n",
      "Configuration saved in ./models/checkpoint-3000/config.json\n",
      "Model weights saved in ./models/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-3500\n",
      "Configuration saved in ./models/checkpoint-3500/config.json\n",
      "Model weights saved in ./models/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-1500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-4000\n",
      "Configuration saved in ./models/checkpoint-4000/config.json\n",
      "Model weights saved in ./models/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-4500\n",
      "Configuration saved in ./models/checkpoint-4500/config.json\n",
      "Model weights saved in ./models/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-2500] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-5000\n",
      "Configuration saved in ./models/checkpoint-5000/config.json\n",
      "Model weights saved in ./models/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3000] due to args.save_total_limit\n",
      "/home/piai/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/checkpoint-5500\n",
      "Configuration saved in ./models/checkpoint-5500/config.json\n",
      "Model weights saved in ./models/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./models/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [models/checkpoint-3500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models/checkpoint-500 (score: 2.9279119968414307).\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding=True, max_length=MAX_LEN,truncation=True)\n",
    "    if 'similar' in examples:\n",
    "        outputs[\"labels\"] = examples[\"similar\"]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "headerColor": "none",
     "id": "b2960977-35fb-4563-a6aa-a738b0f660f3",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-034bfdc908dab38a\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-034bfdc908dab38a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4e1efa87e141588a32adc95365458e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/piai/.cache/huggingface/datasets/csv/default-034bfdc908dab38a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-32e57adf0ac1f496.arrow\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: pair_id. If pair_id are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 179700\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2808' max='2808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2808/2808 30:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TEST = \"./data/test.csv\"\n",
    "SUB = \"./data/sample_submission.csv\"\n",
    "\n",
    "test_dataset = load_dataset(\"csv\", data_files=TEST)[\"train\"]\n",
    "test_dataset = test_dataset.map(example_fn, remove_columns=[\"code1\", \"code2\"])\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "df = pd.read_csv(SUB)\n",
    "df[\"similar\"] = np.argmax(predictions.predictions, axis=-1)\n",
    "df.to_csv(\"./submissions/submission_lv2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
