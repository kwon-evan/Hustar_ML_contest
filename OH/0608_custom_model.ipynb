{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a79954f0-afed-4588-8571-5da6279918d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44cb9f07-5387-4e3b-a778-e7708c4556ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961c7a4a-da35-47e1-8798-0b8631ca86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f31aca-5746-467a-9194-5de289fada40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load RoBERTa Tokenizer\n"
     ]
    }
   ],
   "source": [
    "print(\"Load RoBERTa Tokenizer\")\n",
    "\n",
    "# do_lower_case :대소문자 처리. True시 모두 소문자로 변환함\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\", do_lower_case=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca530e8-0c01-4326-b04a-63baf096780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, RobertaForSequenceClassification, AdamW\n",
    "\n",
    "# pretrained = AutoModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "pretrained = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "\n",
    "# print(pretrained.roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3660138-6659-4d5f-87d0-95067d9552b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([0.64, 0.36]).to(device)\n",
    "loss_fct = torch.nn.CrossEntropyLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bca1517-c0b6-4c8a-aa23-aece74ff7665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-33045f137046f63b\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-33045f137046f63b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd9d6065ea84aa7b39f68764f0fad8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bc8db930d46da531\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-bc8db930d46da531/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a50ef8514d946f59524e72b9b345fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/piai/.cache/huggingface/datasets/csv/default-33045f137046f63b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-8dddc5b5b6246794.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd4c81ce42c4286a756c6561f93f422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# MAX_LEN = 512\n",
    "\n",
    "# def example_fn(examples):\n",
    "#     # padding=True는 0 padding이 안됨. padding=\"max_length\"로 사용하도록.\n",
    "#     # outputs = tokenizer(examples[\"code1\"], examples[\"code2\"], padding=True, max_length=MAX_LEN, truncation=True)\n",
    "#     outputs = tokenizer(examples[\"code1\"], examples[\"code2\"], padding=\"max_length\", max_length=MAX_LEN, truncation=True)\n",
    "#     if \"similar\" in examples:\n",
    "#         outputs[\"labels\"] = examples[\"similar\"]\n",
    "#     return outputs\n",
    "\n",
    "# train_data_lv1 = load_dataset(\"csv\", data_files=\"./data/train_data_300,000.csv\")[\"train\"]\n",
    "# valid_data_lv1 = load_dataset(\"csv\", data_files=\"./data/valid_data_300,000.csv\")[\"train\"]\n",
    "\n",
    "# train_dataset = train_data_lv1.map(example_fn, remove_columns=[\"code1\", \"code2\", \"similar\"])\n",
    "# valid_dataset = valid_data_lv1.map(example_fn, remove_columns=[\"code1\", \"code2\", \"similar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "72ee23c6-0fbb-41c1-b999-17d70577da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# collator가 없으면 data loader가 row와 column을 헷갈려함\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        sampler = torch.utils.data.RandomSampler(train_dataset),\n",
    "        batch_size = batch_size,\n",
    "        collate_fn = data_collator,        \n",
    ")\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler = torch.utils.data.SequentialSampler(valid_dataset),\n",
    "        batch_size = batch_size,\n",
    "        collate_fn = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0f3742b7-3006-4d51-a562-b58007f99893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationHead(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = torch.nn.Linear(768, 3072)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.dense2 = torch.nn.Linear(3072, 768)\n",
    "        self.out_proj = torch.nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # 보통 분류기에선 start 토큰에 분류 결과를 담음\n",
    "        x = features[:, 0, :]    # take <s> token (equiv. to [CLS])\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class MyRobertaModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyRobertaModel, self).__init__()\n",
    "        self.pretrained = pretrained_model\n",
    "        self.classifier = MyClassificationHead()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.pretrained.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # labels=labels\n",
    "        )\n",
    "        self.labels = labels\n",
    "        logits = self.classifier(outputs[\"last_hidden_state\"])\n",
    "        # prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "28690877-fd85-4c80-afe9-955933d4946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRobertaModel(pretrained_model=pretrained)\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "isParallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a3beef61-dc1f-4f77-a25a-a2f6c99d6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dea7daa7-76fa-41d1-b578-cfbb76ae5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c6355b5-7b8d-44e3-abf6-b24dcd04db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu cashe clear\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0e4dd054-8846-43fa-a053-920b0a009c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# gpu cashe clear\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch+1, epochs))\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # batch의 구성이 input_ids, attention_mask, labels로 구성되어 있음\n",
    "        # 그러므로 순서대로 모델에 입력\n",
    "        \n",
    "        # # batch 출력해보기\n",
    "        # print(\"print batch : \")\n",
    "        # print(batch)\n",
    "        batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        logits, loss = model(\n",
    "            input_ids = batch_input_ids,\n",
    "            attention_mask = batch_attention_mask,\n",
    "            labels = batch_labels,\n",
    "        )\n",
    "        \n",
    "        # # output 모양 출력해보기\n",
    "        # outputs = model(\n",
    "        #     input_ids = batch_input_ids,\n",
    "        #     attention_mask = batch_attention_mask,\n",
    "        #     labels = batch_labels,\n",
    "        # )\n",
    "        # print(\"print outputs : \")\n",
    "        # print(outputs)\n",
    "        # print(outputs.last_hidden_state.shape)\n",
    "        # break\n",
    "    \n",
    "        if isParallel:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            print(\"step : {:>5,} of {:>5,} loss: {:.5f}\".format(step, len(train_dataloader), loss.item()))\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print()\n",
    "    print(\" Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "    \n",
    "    # Validation\n",
    "    print()\n",
    "    print(\"Running Validation...\")\n",
    "    \n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(\n",
    "                input_ids = batch_input_ids,\n",
    "                attention_mask = batch_attention_mask,\n",
    "                labels = batch_labels,\n",
    "            )\n",
    "            \n",
    "            if isParallel:\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = batch_labels.to(\"cpu\").numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"Accuracy: {0:.5f}\".format(avg_val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6127326e-c1f9-46b1-8b57-62202712674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "step : 1,000 of 75,000 loss: 0.00547\n",
      "step : 2,000 of 75,000 loss: 0.00133\n",
      "step : 3,000 of 75,000 loss: 0.00446\n",
      "step : 4,000 of 75,000 loss: 0.00127\n",
      "step : 5,000 of 75,000 loss: 0.00332\n",
      "step : 6,000 of 75,000 loss: 0.00083\n",
      "step : 7,000 of 75,000 loss: 0.00060\n",
      "step : 8,000 of 75,000 loss: 0.00037\n",
      "step : 9,000 of 75,000 loss: 0.00041\n",
      "step : 10,000 of 75,000 loss: 0.00672\n",
      "step : 11,000 of 75,000 loss: 0.00181\n",
      "step : 12,000 of 75,000 loss: 0.00072\n",
      "step : 13,000 of 75,000 loss: 0.00045\n",
      "step : 14,000 of 75,000 loss: 0.00035\n",
      "step : 15,000 of 75,000 loss: 0.00043\n",
      "step : 16,000 of 75,000 loss: 0.00046\n",
      "step : 17,000 of 75,000 loss: 0.12599\n",
      "step : 18,000 of 75,000 loss: 0.00347\n",
      "step : 19,000 of 75,000 loss: 0.00055\n",
      "step : 20,000 of 75,000 loss: 0.00090\n",
      "step : 21,000 of 75,000 loss: 0.00084\n",
      "step : 22,000 of 75,000 loss: 0.00146\n",
      "step : 23,000 of 75,000 loss: 0.00062\n",
      "step : 24,000 of 75,000 loss: 0.00065\n",
      "step : 25,000 of 75,000 loss: 0.00147\n",
      "step : 26,000 of 75,000 loss: 0.00052\n",
      "step : 27,000 of 75,000 loss: 0.00092\n",
      "step : 28,000 of 75,000 loss: 0.01973\n",
      "step : 29,000 of 75,000 loss: 0.00043\n",
      "step : 30,000 of 75,000 loss: 0.00144\n",
      "step : 31,000 of 75,000 loss: 1.79532\n",
      "step : 32,000 of 75,000 loss: 0.00066\n",
      "step : 33,000 of 75,000 loss: 0.00041\n",
      "step : 34,000 of 75,000 loss: 0.00039\n",
      "step : 35,000 of 75,000 loss: 0.00086\n",
      "step : 36,000 of 75,000 loss: 0.00035\n",
      "step : 37,000 of 75,000 loss: 0.00032\n",
      "step : 38,000 of 75,000 loss: 0.00074\n",
      "step : 39,000 of 75,000 loss: 0.00044\n",
      "step : 40,000 of 75,000 loss: 0.00289\n",
      "step : 41,000 of 75,000 loss: 0.00044\n",
      "step : 42,000 of 75,000 loss: 0.00039\n",
      "step : 43,000 of 75,000 loss: 0.00057\n",
      "step : 44,000 of 75,000 loss: 0.00118\n",
      "step : 45,000 of 75,000 loss: 0.00160\n",
      "step : 46,000 of 75,000 loss: 0.00047\n",
      "step : 47,000 of 75,000 loss: 0.00099\n",
      "step : 48,000 of 75,000 loss: 0.00205\n",
      "step : 49,000 of 75,000 loss: 0.00034\n",
      "step : 50,000 of 75,000 loss: 0.00038\n",
      "step : 51,000 of 75,000 loss: 0.00026\n",
      "step : 52,000 of 75,000 loss: 0.00053\n",
      "step : 53,000 of 75,000 loss: 0.01323\n",
      "step : 54,000 of 75,000 loss: 0.00385\n",
      "step : 55,000 of 75,000 loss: 0.00059\n",
      "step : 56,000 of 75,000 loss: 0.00146\n",
      "step : 57,000 of 75,000 loss: 0.00043\n",
      "step : 58,000 of 75,000 loss: 0.00043\n",
      "step : 59,000 of 75,000 loss: 0.00058\n",
      "step : 60,000 of 75,000 loss: 0.26218\n",
      "step : 61,000 of 75,000 loss: 0.00043\n",
      "step : 62,000 of 75,000 loss: 0.00050\n",
      "step : 63,000 of 75,000 loss: 0.00050\n",
      "step : 64,000 of 75,000 loss: 0.00410\n",
      "step : 65,000 of 75,000 loss: 0.00024\n",
      "step : 66,000 of 75,000 loss: 0.32707\n",
      "step : 67,000 of 75,000 loss: 0.06023\n",
      "step : 68,000 of 75,000 loss: 0.71302\n",
      "step : 69,000 of 75,000 loss: 0.00679\n",
      "step : 70,000 of 75,000 loss: 0.00063\n",
      "step : 71,000 of 75,000 loss: 0.00047\n",
      "step : 72,000 of 75,000 loss: 0.00038\n",
      "step : 73,000 of 75,000 loss: 0.93383\n",
      "step : 74,000 of 75,000 loss: 0.00599\n",
      "\n",
      " Average training loss: 0.02177\n",
      "\n",
      "Running Validation...\n",
      "Accuracy: 0.99\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "step : 1,000 of 75,000 loss: 0.00042\n",
      "step : 2,000 of 75,000 loss: 0.00037\n",
      "step : 3,000 of 75,000 loss: 0.00063\n",
      "step : 4,000 of 75,000 loss: 0.00153\n",
      "step : 5,000 of 75,000 loss: 0.00479\n",
      "step : 6,000 of 75,000 loss: 0.00034\n",
      "step : 7,000 of 75,000 loss: 0.00094\n",
      "step : 8,000 of 75,000 loss: 0.00054\n",
      "step : 9,000 of 75,000 loss: 0.00132\n",
      "step : 10,000 of 75,000 loss: 0.00052\n",
      "step : 11,000 of 75,000 loss: 0.00846\n",
      "step : 12,000 of 75,000 loss: 0.02387\n",
      "step : 13,000 of 75,000 loss: 0.00128\n",
      "step : 14,000 of 75,000 loss: 0.00042\n",
      "step : 15,000 of 75,000 loss: 0.00087\n",
      "step : 16,000 of 75,000 loss: 0.00050\n",
      "step : 17,000 of 75,000 loss: 0.12920\n",
      "step : 18,000 of 75,000 loss: 0.00068\n",
      "step : 19,000 of 75,000 loss: 0.00059\n",
      "step : 20,000 of 75,000 loss: 0.00052\n",
      "step : 21,000 of 75,000 loss: 0.00075\n",
      "step : 22,000 of 75,000 loss: 0.00042\n",
      "step : 23,000 of 75,000 loss: 0.00050\n",
      "step : 24,000 of 75,000 loss: 0.00078\n",
      "step : 25,000 of 75,000 loss: 0.00052\n",
      "step : 26,000 of 75,000 loss: 0.00070\n",
      "step : 27,000 of 75,000 loss: 0.02753\n",
      "step : 28,000 of 75,000 loss: 0.00039\n",
      "step : 29,000 of 75,000 loss: 0.00103\n",
      "step : 30,000 of 75,000 loss: 0.00033\n",
      "step : 31,000 of 75,000 loss: 0.00037\n",
      "step : 32,000 of 75,000 loss: 0.00032\n",
      "step : 33,000 of 75,000 loss: 0.00030\n",
      "step : 34,000 of 75,000 loss: 0.00036\n",
      "step : 35,000 of 75,000 loss: 0.00033\n",
      "step : 36,000 of 75,000 loss: 0.00080\n",
      "step : 37,000 of 75,000 loss: 0.00935\n",
      "step : 38,000 of 75,000 loss: 0.00068\n",
      "step : 39,000 of 75,000 loss: 0.00084\n",
      "step : 40,000 of 75,000 loss: 0.01740\n",
      "step : 41,000 of 75,000 loss: 0.00055\n",
      "step : 42,000 of 75,000 loss: 0.00060\n",
      "step : 43,000 of 75,000 loss: 0.00046\n",
      "step : 44,000 of 75,000 loss: 0.00053\n",
      "step : 45,000 of 75,000 loss: 0.00066\n",
      "step : 46,000 of 75,000 loss: 0.00045\n",
      "step : 47,000 of 75,000 loss: 0.00036\n",
      "step : 48,000 of 75,000 loss: 0.00071\n",
      "step : 49,000 of 75,000 loss: 0.00049\n",
      "step : 50,000 of 75,000 loss: 0.00075\n",
      "step : 51,000 of 75,000 loss: 0.00070\n",
      "step : 52,000 of 75,000 loss: 0.00043\n",
      "step : 53,000 of 75,000 loss: 0.00050\n",
      "step : 54,000 of 75,000 loss: 0.00254\n",
      "step : 55,000 of 75,000 loss: 0.00062\n",
      "step : 56,000 of 75,000 loss: 0.00066\n",
      "step : 57,000 of 75,000 loss: 0.00049\n",
      "step : 58,000 of 75,000 loss: 0.00039\n",
      "step : 59,000 of 75,000 loss: 0.00053\n",
      "step : 60,000 of 75,000 loss: 0.00060\n",
      "step : 61,000 of 75,000 loss: 0.00055\n",
      "step : 62,000 of 75,000 loss: 0.00048\n",
      "step : 63,000 of 75,000 loss: 0.00045\n",
      "step : 64,000 of 75,000 loss: 0.00048\n",
      "step : 65,000 of 75,000 loss: 0.00113\n",
      "step : 66,000 of 75,000 loss: 0.00040\n",
      "step : 67,000 of 75,000 loss: 0.00050\n",
      "step : 68,000 of 75,000 loss: 0.00049\n",
      "step : 69,000 of 75,000 loss: 0.00081\n",
      "step : 70,000 of 75,000 loss: 0.00399\n",
      "step : 71,000 of 75,000 loss: 0.00034\n",
      "step : 72,000 of 75,000 loss: 0.00038\n",
      "step : 73,000 of 75,000 loss: 0.02152\n",
      "step : 74,000 of 75,000 loss: 0.00065\n",
      "\n",
      " Average training loss: 0.01920\n",
      "\n",
      "Running Validation...\n",
      "Accuracy: 0.99\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "step : 1,000 of 75,000 loss: 0.00037\n",
      "step : 2,000 of 75,000 loss: 0.00028\n",
      "step : 3,000 of 75,000 loss: 0.00035\n",
      "step : 4,000 of 75,000 loss: 0.00077\n",
      "step : 5,000 of 75,000 loss: 0.00044\n",
      "step : 6,000 of 75,000 loss: 0.00033\n",
      "step : 7,000 of 75,000 loss: 0.00047\n",
      "step : 8,000 of 75,000 loss: 0.00034\n",
      "step : 9,000 of 75,000 loss: 0.00031\n",
      "step : 10,000 of 75,000 loss: 0.00038\n",
      "step : 11,000 of 75,000 loss: 0.00054\n",
      "step : 12,000 of 75,000 loss: 0.00036\n",
      "step : 13,000 of 75,000 loss: 0.98033\n",
      "step : 14,000 of 75,000 loss: 0.00034\n",
      "step : 15,000 of 75,000 loss: 0.00033\n",
      "step : 16,000 of 75,000 loss: 0.00033\n",
      "step : 17,000 of 75,000 loss: 0.01434\n",
      "step : 18,000 of 75,000 loss: 0.00260\n",
      "step : 19,000 of 75,000 loss: 0.00074\n",
      "step : 20,000 of 75,000 loss: 0.00103\n",
      "step : 21,000 of 75,000 loss: 0.00035\n",
      "step : 22,000 of 75,000 loss: 0.00029\n",
      "step : 23,000 of 75,000 loss: 0.00020\n",
      "step : 24,000 of 75,000 loss: 0.00021\n",
      "step : 25,000 of 75,000 loss: 0.00021\n",
      "step : 26,000 of 75,000 loss: 0.00022\n",
      "step : 27,000 of 75,000 loss: 0.00038\n",
      "step : 28,000 of 75,000 loss: 0.00028\n",
      "step : 29,000 of 75,000 loss: 0.00028\n",
      "step : 30,000 of 75,000 loss: 0.00044\n",
      "step : 31,000 of 75,000 loss: 0.00034\n",
      "step : 32,000 of 75,000 loss: 0.00049\n",
      "step : 33,000 of 75,000 loss: 0.00046\n",
      "step : 34,000 of 75,000 loss: 0.02987\n",
      "step : 35,000 of 75,000 loss: 0.00037\n",
      "step : 36,000 of 75,000 loss: 0.00031\n",
      "step : 37,000 of 75,000 loss: 0.00034\n",
      "step : 38,000 of 75,000 loss: 0.00054\n",
      "step : 39,000 of 75,000 loss: 0.00561\n",
      "step : 40,000 of 75,000 loss: 0.00043\n",
      "step : 41,000 of 75,000 loss: 0.00052\n",
      "step : 42,000 of 75,000 loss: 0.00041\n",
      "step : 43,000 of 75,000 loss: 0.00430\n",
      "step : 44,000 of 75,000 loss: 0.00031\n",
      "step : 45,000 of 75,000 loss: 0.00029\n",
      "step : 46,000 of 75,000 loss: 0.01082\n",
      "step : 47,000 of 75,000 loss: 0.09703\n",
      "step : 48,000 of 75,000 loss: 0.00028\n",
      "step : 49,000 of 75,000 loss: 0.00170\n",
      "step : 50,000 of 75,000 loss: 0.03312\n",
      "step : 51,000 of 75,000 loss: 0.00022\n",
      "step : 52,000 of 75,000 loss: 0.00020\n",
      "step : 53,000 of 75,000 loss: 0.00017\n",
      "step : 54,000 of 75,000 loss: 0.00075\n",
      "step : 55,000 of 75,000 loss: 0.08074\n",
      "step : 56,000 of 75,000 loss: 0.01927\n",
      "step : 57,000 of 75,000 loss: 0.00040\n",
      "step : 58,000 of 75,000 loss: 0.00038\n",
      "step : 59,000 of 75,000 loss: 0.00048\n",
      "step : 60,000 of 75,000 loss: 0.00039\n",
      "step : 61,000 of 75,000 loss: 0.01927\n",
      "step : 62,000 of 75,000 loss: 0.00065\n",
      "step : 63,000 of 75,000 loss: 0.00062\n",
      "step : 64,000 of 75,000 loss: 0.00055\n",
      "step : 65,000 of 75,000 loss: 0.00424\n",
      "step : 66,000 of 75,000 loss: 0.00074\n",
      "step : 67,000 of 75,000 loss: 0.00071\n",
      "step : 68,000 of 75,000 loss: 0.00072\n",
      "step : 69,000 of 75,000 loss: 0.00040\n",
      "step : 70,000 of 75,000 loss: 0.00043\n",
      "step : 71,000 of 75,000 loss: 0.00060\n",
      "step : 72,000 of 75,000 loss: 0.00053\n",
      "step : 73,000 of 75,000 loss: 0.00050\n",
      "step : 74,000 of 75,000 loss: 0.00044\n",
      "\n",
      " Average training loss: 0.01406\n",
      "\n",
      "Running Validation...\n",
      "Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "model_save_name = \"saved_model.pt\"\n",
    "path = \"./models/\" + model_save_name\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd7d2a-5c65-48c4-a351-140e4c6bcbb9",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7e49ae6d-71e3-46cf-bd68-d4f2c6da8642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MyRobertaModel(\n",
       "    (pretrained): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): RobertaClassificationHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (classifier): MyClassificationHead(\n",
       "      (dense1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dense2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./models/saved_model.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "05e75d20-87c6-4f14-aab7-646ab022398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e82830c223845fb2\n",
      "Reusing dataset csv (/home/piai/.cache/huggingface/datasets/csv/default-e82830c223845fb2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7db93bc7ff463593ca837799a99eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/piai/.cache/huggingface/datasets/csv/default-e82830c223845fb2/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-57bb7ab9e51e8a9b.arrow\n"
     ]
    }
   ],
   "source": [
    "TEST = \"./data/test.csv\"\n",
    "SUB = \"./data/sample_submission.csv\"\n",
    "\n",
    "test_dataset = load_dataset(\"csv\", data_files=TEST)[\"train\"]\n",
    "test_dataset = test_dataset.map(example_fn, remove_columns=[\"code1\", \"code2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2cb70e05-e5e6-499e-8106-fba6e5fd7d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pair_id', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 179700\n",
       "})"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8f04d745-066a-4401-9e03-750b69889c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        sampler = torch.utils.data.SequentialSampler(test_dataset),\n",
    "        batch_size = 1,\n",
    "        collate_fn = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a25126f3-1882-4628-80cd-35ac487c3e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 179700/179700 [1:28:24<00:00, 33.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = list()\n",
    "for idx, element in enumerate(tqdm(test_dataloader)):\n",
    "    element_input_ids = element[\"input_ids\"].to(device)\n",
    "    element_attention_mask = element[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # print(element_input_ids.shape)\n",
    "    # print(element_attention_mask.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids = element_input_ids,\n",
    "            attention_mask = element_attention_mask,\n",
    "        )\n",
    "        \n",
    "        # print(logits)        \n",
    "        prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # print(prob)\n",
    "        predict = torch.argmax(prob, axis=1)\n",
    "        # print(predict)\n",
    "        predict = predict.detach().cpu().numpy()\n",
    "        predictions.append(predict[0])\n",
    "    \n",
    "    # # 시범 출력\n",
    "#     if idx > 10:\n",
    "#         break\n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "60b89eed-7800-449c-9dc1-19c3800a2839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        pair_id  similar\n",
      "0             1        0\n",
      "1             2        1\n",
      "2             3        0\n",
      "3             4        0\n",
      "4             5        0\n",
      "...         ...      ...\n",
      "179695   179696        1\n",
      "179696   179697        1\n",
      "179697   179698        0\n",
      "179698   179699        1\n",
      "179699   179700        1\n",
      "\n",
      "[179700 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(SUB)\n",
    "df[\"similar\"] = predictions\n",
    "print(df)\n",
    "df.to_csv(\"./submissions/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc5bd8-8e9f-4483-819c-f92db65c812d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
